{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyh26x03/aml-redflags-rag/blob/main/experiment_rag_v2_display.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT_QsrdheWwa"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"experiment_rag_v2\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1vYowV_H0PHjKktn2fqphP-dmEnegB_z-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWvKTUYHd1Om"
      },
      "source": [
        "# ğŸ”§ PART 0: SETUPï¼ˆç’°å¢ƒè¨­å®šï¼‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Hh8kg7ef8G"
      },
      "source": [
        "System Version: **v2**\n",
        "\n",
        "History:\n",
        "- v1: åŸºç¤ RAGï¼ˆDense + BM25 + RRF â†’ LLMï¼‰\n",
        "- v2: metadata åˆ†å±¤ã€priority weightingã€Pre-LLM Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tYhbdzFeFCV"
      },
      "source": [
        "### 0.1 å®‰è£å¥—ä»¶\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgmniIRAdbSN"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==2.2.2 numpy==2.0.2 pypdf langchain-text-splitters sentence-transformers faiss-cpu rank_bm25 google-generativeai huggingface_hub --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkpuGHNn5dqJ"
      },
      "outputs": [],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqELrbVzeNQ1"
      },
      "source": [
        "### 0.2 Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVsDMr8YeO0e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi__XUlneRLW"
      },
      "source": [
        "### 0.3 è¨­å®šè·¯å¾‘èˆ‡ API Key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDDYpJF-5dWl"
      },
      "outputs": [],
      "source": [
        "# --- è·¯å¾‘ ---\n",
        "INDEX_DIR = \"/content/drive/MyDrive/AML/index_v2/v2\"\n",
        "EXPERIMENT_ROOT_DIR = \"/content/drive/MyDrive/AML/experiments/runs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE-eompmmlke"
      },
      "outputs": [],
      "source": [
        "# --- Embedding æ¨¡å‹ï¼ˆfallbackï¼Œæ­£å¸¸æƒ…æ³å¾ metadata.json è®€å–ï¼‰---\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8i6_9uYe5QD"
      },
      "outputs": [],
      "source": [
        "# --- æª¢ç´¢åƒæ•¸ ---\n",
        "DEFAULT_TOP_K = 5\n",
        "DEFAULT_RRF_K = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuIM5lp7e79t"
      },
      "outputs": [],
      "source": [
        "# --- åŠŸèƒ½é–‹é—œ ---\n",
        "DEFAULT_USE_PRIORITY_WEIGHTING = True\n",
        "DEFAULT_ENABLE_GATE = True   # Gate é è¨­é—œé–‰ï¼Œå¯¦é©—æ™‚æ‰‹å‹•é–‹å•Ÿ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhDjqUYqmt_z"
      },
      "outputs": [],
      "source": [
        "# --- API Key ---\n",
        "from google.colab import userdata\n",
        "GROQ_API_KEY = userdata.get(\"colab-rag-llama3-dev\")\n",
        "GEMINI_API_KEY = userdata.get(\"aml-redflag-auditable-rag\")\n",
        "# HF_TOKEN = userdata.get('rag-demo-qwen-test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2nyULXRfA01"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# --- é©—è­‰ ---\n",
        "assert os.path.exists(INDEX_DIR), f\"âŒ ç´¢å¼•è·¯å¾‘ä¸å­˜åœ¨: {INDEX_DIR}\"\n",
        "print(f\"âœ… Config è¼‰å…¥å®Œæˆ\")\n",
        "print(f\"   ç´¢å¼•è·¯å¾‘: {INDEX_DIR}\")\n",
        "print(f\"   Priority Weighting: {DEFAULT_USE_PRIORITY_WEIGHTING}\")\n",
        "print(f\"   Gate: {DEFAULT_ENABLE_GATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkxDeYhW5t7Y"
      },
      "source": [
        "###0.4 LLM æ¨¡å‹é¸æ“‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AViOOF3nahy9"
      },
      "outputs": [],
      "source": [
        "print(\"\\né¸æ“‡ LLM æ¨¡å‹ï¼š\")\n",
        "print(\"  [1] Llama-3.3-70B (Groq) ğŸš€ â€” æœ€å¼·æ¨ç†ï¼Œé©åˆæœ€çµ‚ç”¢å‡º\")\n",
        "print(\"  [2] Gemini Flash â€” å¹³è¡¡å‹\")\n",
        "print(\"  [3] Gemini Flash Lite â€” å¿«é€Ÿè¼•é‡\")\n",
        "print(\"  [4] Llama-3.1-8B (Groq) âš¡ â€” æª¢æ ¸å·¥å…·ï¼šæ¸¬è©¦è³‡æ–™æª¢ç´¢ç²¾æº–åº¦\")\n",
        "print(\"  [5] Mixtral-8x7b (Groq) ğŸ§  â€” ä¸­å …å¯¦åŠ›ï¼šæ¸¬è©¦é‚è¼¯ä¸­è½‰\")\n",
        "\n",
        "MODEL_CHOICE = input(\"è¼¸å…¥ 1/2/3/4/5: \").strip() or \"1\"\n",
        "\n",
        "LLM_CONFIG = {\n",
        "    \"1\": {\n",
        "        \"provider\": \"groq\",\n",
        "        \"llm_model_name\": \"llama-3.3-70b-versatile\",\n",
        "        \"api_key\": GROQ_API_KEY,\n",
        "    },\n",
        "    \"2\": {\n",
        "        \"provider\": \"gemini\",\n",
        "        \"llm_model_name\": \"gemini-2.0-flash\",\n",
        "        \"api_key\": GEMINI_API_KEY,\n",
        "    },\n",
        "    \"3\": {\n",
        "        \"provider\": \"gemini\",\n",
        "        \"llm_model_name\": \"gemini-2.0-flash-lite\",\n",
        "        \"api_key\": GEMINI_API_KEY,\n",
        "    },\n",
        "    \"4\": {\n",
        "        \"provider\": \"groq\",\n",
        "        \"llm_model_name\": \"llama-3.1-8b-instant\",\n",
        "        \"api_key\": GROQ_API_KEY,\n",
        "    },\n",
        "    \"5\": {\n",
        "        \"provider\": \"groq\",\n",
        "        \"llm_model_name\": \"mixtral-8x7b-32768\",\n",
        "        \"api_key\": GROQ_API_KEY,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y_4r7c79omw"
      },
      "outputs": [],
      "source": [
        "SELECTED_CONFIG = LLM_CONFIG[MODEL_CHOICE]\n",
        "print(f\"âœ… å·²é¸æ“‡ï¼š{SELECTED_CONFIG['llm_model_name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys_OH8AuQCHu"
      },
      "source": [
        "### 0.5 å…±ç”¨ Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bYwgpMy6EMk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Set\n",
        "from enum import Enum\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import jieba\n",
        "import google.generativeai as genai\n",
        "from groq import Groq\n",
        "\n",
        "print(\"âœ… æ‰€æœ‰ imports å®Œæˆ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r5lj-wC6FT8"
      },
      "source": [
        "### 0.6 ğŸ“Š å¯¦é©—è¨˜éŒ„å™¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3cLDxRFDvDH"
      },
      "outputs": [],
      "source": [
        "\"\"\"### 0.6 âœï¸ å¯¦é©—è¨­å®šï¼ˆæ¯æ¬¡å¯¦é©—å‰æ”¹é€™æ ¼ï¼‰\"\"\"\n",
        "\n",
        "# ============================================\n",
        "# âœï¸ æ¯æ¬¡å¯¦é©—å‰åªæ”¹é€™æ ¼\n",
        "# ============================================\n",
        "\n",
        "EXPERIMENT_VERSION = \"v2.1\"   # â† ç‰ˆæœ¬è™Ÿ\n",
        "EXPERIMENT_TYPE = \"fix\"    # â† baseline / fix / tuning / ab_test / debug\n",
        "EXPERIMENT_NOTE = \"expand_KS\"    # â† è‡ªç”±å‚™è¨»ï¼ˆæœƒå¯«é€²ç›®éŒ„åå’Œ metadataï¼‰\n",
        "ENABLE_LOGGING = True   # â† æ˜¯å¦è¨˜éŒ„åˆ° Google Drive\n",
        "\n",
        "# EXPERIMENT_VERSION = \"v2.0\"   # â† ç‰ˆæœ¬è™Ÿ\n",
        "# EXPERIMENT_TYPE = \"baseline\"    # â† baseline / fix / tuning / ab_test / debug\n",
        "# EXPERIMENT_NOTE = \"\"    # â† è‡ªç”±å‚™è¨»ï¼ˆæœƒå¯«é€²ç›®éŒ„åå’Œ metadataï¼‰\n",
        "# ENABLE_LOGGING = True   # â† æ˜¯å¦è¨˜éŒ„åˆ° Google Drive\n",
        "\n",
        "# è¡Œç‚ºé–‹é—œ\n",
        "USE_PRIORITY_WEIGHTING = True   # â† æ˜¯å¦å•Ÿç”¨ priority weighting\n",
        "ENABLE_GATE = True    # â† æ˜¯å¦å•Ÿç”¨ Pre-LLM Gate\n",
        "\n",
        "print(f\"ğŸ“‹ å¯¦é©—è¨­å®š: {EXPERIMENT_VERSION} / {EXPERIMENT_TYPE}\")\n",
        "print(f\"   Note: {EXPERIMENT_NOTE or '(none)'}\")\n",
        "print(f\"   Priority={USE_PRIORITY_WEIGHTING} | Gate={ENABLE_GATE} | Log={ENABLE_LOGGING}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYyBNDlrD1x9"
      },
      "outputs": [],
      "source": [
        "\"\"\"### 0.7 ğŸ“Š å¯¦é©—è¨˜éŒ„å™¨\"\"\"\n",
        "\n",
        "class ExperimentLogger:\n",
        "    \"\"\"RAG å¯¦é©—è¨˜éŒ„å™¨\n",
        "\n",
        "    è·è²¬ï¼šæŠŠæ¯æ¬¡å¯¦é©—çš„è¨­å®šã€çµæœã€æŒ‡æ¨™å¯«æˆ JSON å­˜åˆ° Google Driveã€‚\n",
        "    ä¸è² è²¬æ±ºå®šå¯¦é©—åƒæ•¸ï¼ˆé‚£æ˜¯ Cell 0.6 çš„äº‹ï¼‰ã€‚\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_dir: str = EXPERIMENT_ROOT_DIR,\n",
        "        version: str = EXPERIMENT_VERSION,\n",
        "        experiment_type: str = EXPERIMENT_TYPE,\n",
        "        note: str = EXPERIMENT_NOTE,\n",
        "    ):\n",
        "        date_str = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "        # ç›®éŒ„åï¼šv2.0_20250210_baseline_some_note\n",
        "        exp_name = f\"{version}_{date_str}_{experiment_type}\"\n",
        "        if note:\n",
        "            exp_name += f\"_{note}\"\n",
        "\n",
        "        self.exp_dir = Path(base_dir) / exp_name\n",
        "        self.cases_dir = self.exp_dir / \"cases\"\n",
        "        self.exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.cases_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        self.metadata = {\n",
        "            \"experiment_id\": exp_name,\n",
        "            \"version\": version,\n",
        "            \"type\": experiment_type,\n",
        "            \"note\": note,\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "            \"status\": \"running\",\n",
        "        }\n",
        "\n",
        "        print(f\"ğŸ“ å¯¦é©—ç›®éŒ„: {exp_name}\")\n",
        "\n",
        "    def log_config(self, config: Dict[str, Any]):\n",
        "        \"\"\"è¨˜éŒ„é…ç½®\"\"\"\n",
        "        with open(self.exp_dir / \"config.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"âœ… é…ç½®å·²ä¿å­˜\")\n",
        "\n",
        "    def log_case(\n",
        "        self,\n",
        "        case_id: str,\n",
        "        scenario: str,\n",
        "        expected: str,\n",
        "        actual: str,\n",
        "        passed: bool,\n",
        "        retrieved_chunks: List[Dict] = None,\n",
        "        llm_response: Dict = None,\n",
        "        error: str = None,\n",
        "    ):\n",
        "        \"\"\"è¨˜éŒ„å–®å€‹æ¡ˆä¾‹\"\"\"\n",
        "        case_data = {\n",
        "            \"case_id\": case_id,\n",
        "            \"scenario\": scenario,\n",
        "            \"expected\": expected,\n",
        "            \"actual\": actual,\n",
        "            \"passed\": passed,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "        }\n",
        "        if retrieved_chunks:\n",
        "            case_data[\"retrieved_chunks\"] = retrieved_chunks\n",
        "        if llm_response:\n",
        "            case_data[\"llm_response\"] = llm_response\n",
        "        if error:\n",
        "            case_data[\"error\"] = error\n",
        "\n",
        "        status = \"pass\" if passed else \"fail\"\n",
        "        with open(self.cases_dir / f\"{case_id}_{expected}_{status}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(case_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        emoji = \"âœ…\" if passed else \"âŒ\"\n",
        "        print(f\"  {emoji} {case_id}: {expected} â†’ {actual}\")\n",
        "\n",
        "    def log_batch_results(self, results: List[Dict[str, Any]]):\n",
        "        \"\"\"è¨˜éŒ„æ‰¹æ¬¡çµæœ\"\"\"\n",
        "        with open(self.exp_dir / \"results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"âœ… æ‰¹æ¬¡çµæœå·²ä¿å­˜\")\n",
        "\n",
        "    def log_metrics(self, metrics: Dict[str, Any]):\n",
        "        \"\"\"è¨˜éŒ„æŒ‡æ¨™\"\"\"\n",
        "        with open(self.exp_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"âœ… æŒ‡æ¨™å·²ä¿å­˜\")\n",
        "\n",
        "    def finalize(self, summary: str = \"\"):\n",
        "        \"\"\"å®Œæˆå¯¦é©—\"\"\"\n",
        "        self.metadata[\"status\"] = \"completed\"\n",
        "        self.metadata[\"completed_at\"] = datetime.now().isoformat()\n",
        "        if summary:\n",
        "            self.metadata[\"summary\"] = summary\n",
        "\n",
        "        with open(self.exp_dir / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\nğŸ‰ å¯¦é©—å®Œæˆ: {self.exp_dir.name}\")\n",
        "        print(f\"ğŸ“Š {summary}\")\n",
        "\n",
        "print(\"âœ… ExperimentLogger å·²å®šç¾©\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEemm8RQ6Qkg"
      },
      "outputs": [],
      "source": [
        "# class ExperimentLogger:\n",
        "#     \"\"\"RAG å¯¦é©—è¨˜éŒ„å™¨\"\"\"\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         base_dir: str = EXPERIMENT_ROOT_DIR,\n",
        "#         version: str = \"v2.0\",\n",
        "#         experiment_type: str = \"baseline\",\n",
        "#         description: str = \"\",\n",
        "#         config: Dict[str, Any] = None,\n",
        "#         auto_describe: bool = True,\n",
        "#     ):\n",
        "#         \"\"\"åˆå§‹åŒ–å¯¦é©—è¨˜éŒ„å™¨\"\"\"\n",
        "#         date_str = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "#         if not description and auto_describe and config:\n",
        "#             description = self._auto_generate_description(config, experiment_type)\n",
        "\n",
        "#         exp_name = f\"{version}_{date_str}_{experiment_type}\"\n",
        "#         if description:\n",
        "#             exp_name += f\"_{description}\"\n",
        "\n",
        "#         self.exp_dir = Path(base_dir) / exp_name\n",
        "#         self.cases_dir = self.exp_dir / \"cases\"\n",
        "\n",
        "#         self.exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "#         self.cases_dir.mkdir(exist_ok=True)\n",
        "\n",
        "#         self.metadata = {\n",
        "#             \"experiment_id\": exp_name,\n",
        "#             \"version\": version,\n",
        "#             \"type\": experiment_type,\n",
        "#             \"description\": description,\n",
        "#             \"created_at\": datetime.now().isoformat(),\n",
        "#             \"status\": \"running\",\n",
        "#         }\n",
        "\n",
        "#         print(f\"ğŸ“ å¯¦é©—ç›®éŒ„: {exp_name}\")\n",
        "\n",
        "#     def _auto_generate_description(self, config: Dict[str, Any], exp_type: str) -> str:\n",
        "#         \"\"\"è‡ªå‹•å¾é…ç½®ç”Ÿæˆæè¿°\"\"\"\n",
        "#         model_map = {\n",
        "#             \"llama-3.3-70b-versatile\": \"L70b\",\n",
        "#             \"llama-3.1-8b-instant\": \"L8b\",\n",
        "#             \"gemini-2.0-flash\": \"GemF\",\n",
        "#             \"gemini-2.0-flash-lite\": \"GemFL\",\n",
        "#             \"mixtral-8x7b-32768\": \"Mix8x7\",\n",
        "#         }\n",
        "#         raw_name = config.get(\"llm_model_name\", \"unknown\")\n",
        "#         model_tag = model_map.get(raw_name, \"other\")\n",
        "\n",
        "#         if exp_type == \"baseline\":\n",
        "#             return model_tag\n",
        "#         elif exp_type == \"fix\":\n",
        "#             if config.get(\"use_priority_weighting\"):\n",
        "#                 return f\"{model_tag}_priority_enabled\"\n",
        "#             return f\"{model_tag}_bug_fix\"\n",
        "#         elif exp_type == \"tuning\":\n",
        "#             changed = [model_tag]\n",
        "#             if config.get(\"k\", 5) != 5:\n",
        "#                 changed.append(f\"k{config['k']}\")\n",
        "#             return \"_\".join(changed[:3])\n",
        "#         elif exp_type == \"ab_test\":\n",
        "#             return f\"{model_tag}_gate_comparison\"\n",
        "#         elif exp_type == \"debug\":\n",
        "#             if \"target_case\" in config:\n",
        "#                 return f\"{model_tag}_case_{config['target_case']}\"\n",
        "#             return f\"{model_tag}_investigation\"\n",
        "#         return model_tag\n",
        "\n",
        "#     def log_config(self, config: Dict[str, Any]):\n",
        "#         \"\"\"è¨˜éŒ„é…ç½®\"\"\"\n",
        "#         config_path = self.exp_dir / \"config.json\"\n",
        "#         with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "#             json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "#         print(f\"âœ… é…ç½®å·²ä¿å­˜\")\n",
        "\n",
        "#     def log_case(\n",
        "#         self,\n",
        "#         case_id: str,\n",
        "#         scenario: str,\n",
        "#         expected: str,\n",
        "#         actual: str,\n",
        "#         passed: bool,\n",
        "#         retrieved_chunks: List[Dict] = None,\n",
        "#         llm_response: Dict = None,\n",
        "#         error: str = None,\n",
        "#     ):\n",
        "#         \"\"\"è¨˜éŒ„å–®å€‹æ¡ˆä¾‹\"\"\"\n",
        "#         case_data = {\n",
        "#             \"case_id\": case_id,\n",
        "#             \"scenario\": scenario,\n",
        "#             \"expected\": expected,\n",
        "#             \"actual\": actual,\n",
        "#             \"passed\": passed,\n",
        "#             \"timestamp\": datetime.now().isoformat(),\n",
        "#         }\n",
        "\n",
        "#         if retrieved_chunks:\n",
        "#             case_data[\"retrieved_chunks\"] = retrieved_chunks\n",
        "#         if llm_response:\n",
        "#             case_data[\"llm_response\"] = llm_response\n",
        "#         if error:\n",
        "#             case_data[\"error\"] = error\n",
        "\n",
        "#         status = \"pass\" if passed else \"fail\"\n",
        "#         case_path = self.cases_dir / f\"{case_id}_{expected}_{status}.json\"\n",
        "\n",
        "#         with open(case_path, \"w\", encoding=\"utf-8\") as f:\n",
        "#             json.dump(case_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "#         emoji = \"âœ…\" if passed else \"âŒ\"\n",
        "#         print(f\"  {emoji} {case_id}: {expected} â†’ {actual}\")\n",
        "\n",
        "#     def log_batch_results(self, results: List[Dict[str, Any]]):\n",
        "#         \"\"\"è¨˜éŒ„æ‰¹æ¬¡çµæœ\"\"\"\n",
        "#         results_path = self.exp_dir / \"results.json\"\n",
        "#         with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "#             json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "#         print(f\"âœ… æ‰¹æ¬¡çµæœå·²ä¿å­˜\")\n",
        "\n",
        "#     def log_metrics(self, metrics: Dict[str, Any]):\n",
        "#         \"\"\"è¨˜éŒ„æŒ‡æ¨™\"\"\"\n",
        "#         metrics_path = self.exp_dir / \"metrics.json\"\n",
        "#         with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "#             json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
        "#         print(f\"âœ… æŒ‡æ¨™å·²ä¿å­˜\")\n",
        "\n",
        "#     def finalize(self, summary: str = \"\"):\n",
        "#         \"\"\"å®Œæˆå¯¦é©—\"\"\"\n",
        "#         self.metadata[\"status\"] = \"completed\"\n",
        "#         self.metadata[\"completed_at\"] = datetime.now().isoformat()\n",
        "#         if summary:\n",
        "#             self.metadata[\"summary\"] = summary\n",
        "\n",
        "#         meta_path = self.exp_dir / \"metadata.json\"\n",
        "#         with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "#             json.dump(self.metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "#         print(f\"\\nğŸ‰ å¯¦é©—å®Œæˆ: {self.exp_dir.name}\")\n",
        "#         print(f\"ğŸ“Š {summary}\")\n",
        "\n",
        "# print(\"âœ… ExperimentLogger å·²å®šç¾©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hsn9o1v6Wc8"
      },
      "source": [
        "# ğŸ“‹ PART 1: Pre-LLM Gateï¼ˆçŸ¥è­˜ç¯„åœå®ˆé–€ï¼‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0syE6B3f1nc"
      },
      "source": [
        "åœ¨ LLM åˆ†æå‰é€²è¡Œ deterministic åˆ¤æ–·ï¼š\n",
        "- ä¸»é¡Œæ˜¯å¦åœ¨çŸ¥è­˜ç¯„åœå…§ï¼Ÿ\n",
        "- æ˜¯å¦æœ‰æ˜ç¢ºçŸ¥è­˜ç¼ºå£ï¼Ÿ\n",
        "- è­‰æ“šæ˜¯å¦è¶³å¤ ï¼Ÿ\n",
        "\n",
        "> Gate æ˜¯å¯é–‹é—œçš„ï¼ˆ`enable_gate` åƒæ•¸ï¼‰ï¼Œé è¨­é—œé–‰ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q17sx7vy6Zod"
      },
      "outputs": [],
      "source": [
        "class KnowledgeManifest:\n",
        "    \"\"\"çŸ¥è­˜æ¸…å–®ï¼šå®šç¾©ç³»çµ±çŸ¥è­˜ç¯„åœ\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.covered_topics: Dict[str, str] = {\n",
        "            \"virtual_assets\": \"è™›æ“¬è³‡ç”¢/åŠ å¯†è²¨å¹£ç›¸é—œç´…æ——\",\n",
        "            \"cash_structuring\": \"ç¾é‡‘æ‹†åˆ†/é–€æª»è¦é¿\",\n",
        "            \"rapid_movement\": \"å¿«é€Ÿæµè½‰/éè·¯å¸³æˆ¶\",\n",
        "            \"third_party\": \"ç¬¬ä¸‰äººä»£è¾¦/äººé ­å¸³æˆ¶\",\n",
        "            \"cross_border\": \"è·¨å¢ƒé«˜é¢¨éšªåœ°å€\",\n",
        "            \"identity_mismatch\": \"èˆ‡èº«åˆ†/å•†æ¥­æ¨¡å¼ä¸ç¬¦\",\n",
        "            \"shell_company\": \"ç©ºæ®¼å…¬å¸/å—ç›Šäººä¸é€æ˜\",\n",
        "        }\n",
        "\n",
        "        self.not_covered_topics: Dict[str, str] = {\n",
        "            \"TBML\": \"è²¿æ˜“å‹æ´—éŒ¢ï¼ˆTrade-Based Money Launderingï¼‰\",\n",
        "            \"sanctions\": \"åˆ¶è£åå–®ç¯©é¸\",\n",
        "            \"tax_evasion\": \"ç¨…å‹™é€ƒæ¼\",\n",
        "        }\n",
        "\n",
        "        self.required_evidence: Dict[str, List[str]] = {\n",
        "            \"TBML\": [\"invoice\", \"customs\", \"shipping\", \"goods_flow\"],\n",
        "        }\n",
        "\n",
        "    def is_topic_covered(self, topic: str) -> bool:\n",
        "        return topic.lower() in [t.lower() for t in self.covered_topics.keys()]\n",
        "\n",
        "    def is_topic_explicitly_not_covered(self, topic: str) -> bool:\n",
        "        return topic.upper() in [t.upper() for t in self.not_covered_topics.keys()]\n",
        "\n",
        "    def get_required_evidence(self, topic: str) -> List[str]:\n",
        "        return self.required_evidence.get(topic.upper(), [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlZjpb876b31"
      },
      "outputs": [],
      "source": [
        "class TopicDetector:\n",
        "    \"\"\"ä¸»é¡Œåµæ¸¬å™¨ï¼šrule-based é—œéµå­—åŒ¹é…\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.topic_keywords: Dict[str, List[str]] = {\n",
        "            \"TBML\": [\n",
        "                \"è²¿æ˜“å‹æ´—éŒ¢\", \"TBML\", \"å ±é—œ\", \"å ±é—œå–®\", \"ç™¼ç¥¨\",\n",
        "                \"è²¨ç‰©\", \"è²¨ç‰©æµå‘\", \"é€²å‡ºå£\", \"åœ‹éš›è²¿æ˜“\",\n",
        "                \"ä¿¡ç”¨ç‹€\", \"L/C\", \"æå–®\", \"invoice\",\n",
        "                \"trade-based\", \"trade based\", \"customs\", \"shipping\",\n",
        "            ],\n",
        "            \"virtual_assets\": [\n",
        "                \"è™›æ“¬è³‡ç”¢\", \"åŠ å¯†è²¨å¹£\", \"æ¯”ç‰¹å¹£\", \"ä»¥å¤ªå¹£\",\n",
        "                \"æ··å¹£\", \"éŒ¢åŒ…\", \"äº¤æ˜“æ‰€\", \"OTC\", \"ç§ä¸‹äº¤æ˜“\",\n",
        "                \"éè¨—ç®¡\", \"éš±ç§å¹£\",\n",
        "            ],\n",
        "            \"cash_structuring\": [\n",
        "                \"æ‹†åˆ†\", \"é–€æª»\", \"å°é¡\", \"å¤šç­†\", \"ç”³å ±\",\n",
        "            ],\n",
        "            \"rapid_movement\": [\n",
        "                \"å¿«é€Ÿæµè½‰\", \"å…¥å¸³å³è½‰\", \"éè·¯å¸³æˆ¶\", \"å¤šå°æ‰‹æ–¹\",\n",
        "                \"å¾ˆå¿«è½‰å‡º\", \"ç«‹å³è½‰å‡º\", \"è½‰å‡º\", \"é€²å‡ºé »ç¹\",   # æ–°å¢å£èªåŒ–æè¿°\n",
        "            ],\n",
        "            \"third_party\": [\n",
        "                \"ç¬¬ä¸‰äºº\", \"ä»£è¾¦\", \"äººé ­\", \"ä»£ç‚ºæ“ä½œ\",\n",
        "            ],\n",
        "            \"cross_border\": [\n",
        "                \"è·¨å¢ƒ\", \"å¢ƒå¤–\", \"é«˜é¢¨éšªåœ°å€\", \"åŒ¯å¾€\", \"åŒ¯æ¬¾\",\n",
        "            ],\n",
        "            \"identity_mismatch\": [\n",
        "                \"èˆ‡èº«åˆ†ä¸ç¬¦\", \"èˆ‡è·æ¥­ä¸ç¬¦\", \"èˆ‡æ¥­å‹™ä¸ç¬¦\",\n",
        "                \"æ¥­å‹™ç„¡é—œ\", \"ä¸ç¬¦\", \"ç„¡é—œ\", \"ç„¡æ³•èªªæ˜\",     # new\n",
        "            ],\n",
        "            \"shell_company\": [\n",
        "                \"ç©ºæ®¼å…¬å¸\", \"å—ç›Šäººä¸æ˜\", \"è‚¡æ¬Šçµæ§‹\", \"ä¸é€æ˜\",\n",
        "            ],\n",
        "        }\n",
        "\n",
        "    def detect_topics(self, text: str) -> Set[str]:\n",
        "        detected = set()\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for topic, keywords in self.topic_keywords.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword.lower() in text_lower:\n",
        "                    detected.add(topic)\n",
        "                    break\n",
        "\n",
        "        return detected\n",
        "\n",
        "    def detect_evidence_fields(self, text: str) -> Set[str]:\n",
        "        evidence_keywords = {\n",
        "            \"invoice\": [\"ç™¼ç¥¨\", \"invoice\", \"å–®æ“š\"],\n",
        "            \"customs\": [\"å ±é—œå–®\", \"å ±é—œ\", \"customs\", \"æµ·é—œ\"],\n",
        "            \"shipping\": [\"æå–®\", \"é‹é€\", \"shipping\", \"ç‰©æµ\"],\n",
        "            \"goods_flow\": [\"è²¨ç‰©æµå‘\", \"è²¨ç‰©\", \"å•†å“\", \"goods\"],\n",
        "        }\n",
        "\n",
        "        detected = set()\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for field_name, keywords in evidence_keywords.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword.lower() in text_lower:\n",
        "                    detected.add(field_name)\n",
        "                    break\n",
        "\n",
        "        return detected\n",
        "\n",
        "    def detect_explicit_knowledge_gap(self, text: str) -> bool:\n",
        "        \"\"\"åµæ¸¬ scenario æ˜¯å¦æ˜ç¢ºè¡¨ç¤ºçŸ¥è­˜ç¼ºå£\"\"\"\n",
        "        gap_patterns = [\n",
        "            r\"æ•™ææ²’æœ‰\",\n",
        "            r\"æ²’æœ‰.*ç« ç¯€\",\n",
        "            r\"æ²’æœ‰.*è³‡æ–™\",\n",
        "            r\"æ‰‹ä¸Šæ²’æœ‰\",\n",
        "            r\"ç¼ºä¹.*æ–‡ä»¶\",\n",
        "        ]\n",
        "\n",
        "        for pattern in gap_patterns:\n",
        "            if re.search(pattern, text):\n",
        "                return True\n",
        "\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQaOeRsG6fP8"
      },
      "outputs": [],
      "source": [
        "class GateDecision(Enum):\n",
        "    ALLOW = \"ALLOW\"\n",
        "    REFUSE = \"REFUSE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEtZu-V56jD9"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GateResult:\n",
        "    decision: GateDecision\n",
        "    reason_code: str = \"\"\n",
        "    reason_message: str = \"\"\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    def to_refuse_response(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"scenario_summary\": self.reason_message,\n",
        "            \"assessment\": \"refuse\",\n",
        "            \"identified_flags\": [],\n",
        "            \"follow_up_questions\": [],\n",
        "            \"disclaimer\": \"æœ¬åˆ†æåƒ…ä¾›æ•™è‚²åƒè€ƒï¼Œä¸æ§‹æˆæ³•å¾‹æ„è¦‹ã€‚\",\n",
        "            \"_gate_metadata\": {\n",
        "                \"decision\": self.decision.value,\n",
        "                \"reason_code\": self.reason_code,\n",
        "                \"detected_topics\": self.metadata.get(\"detected_topics\", []),\n",
        "            },\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJFF-BZA8BkW"
      },
      "outputs": [],
      "source": [
        "def pre_llm_gate(\n",
        "    scenario: str,\n",
        "    manifest: KnowledgeManifest = None,\n",
        "    detector: TopicDetector = None,\n",
        "    verbose: bool = False,\n",
        ") -> GateResult:\n",
        "    \"\"\"\n",
        "    Pre-LLM Gateï¼ˆBalanced v3ï¼‰\n",
        "\n",
        "    è¨­è¨ˆå“²å­¸ï¼š\n",
        "    - Gate åªè² è²¬ã€Œç½é›£ç´šæ‹’çµ•ã€\n",
        "    - ç°éšæƒ…å¢ƒä¸€å¾‹æ”¾è¡Œï¼Œä½†ç•™ä¸‹æ˜ç¢ºé¢¨éšªæ¨™è¨˜\n",
        "    - è­‰æ“šä¸è¶³ â†’ è­¦å‘Šï¼ˆwarningï¼‰ï¼Œä¸æ˜¯ veto\n",
        "    \"\"\"\n",
        "\n",
        "    if manifest is None:\n",
        "        manifest = KnowledgeManifest()\n",
        "    if detector is None:\n",
        "        detector = TopicDetector()\n",
        "\n",
        "    detected_topics = detector.detect_topics(scenario)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Rule 1: ä½¿ç”¨è€…æ˜ç¢ºè¡¨ç¤ºæ²’æœ‰è³‡æ–™\n",
        "    # ----------------------------\n",
        "    if detector.detect_explicit_knowledge_gap(scenario):\n",
        "        uncovered = [\n",
        "            t for t in detected_topics\n",
        "            if manifest.is_topic_explicitly_not_covered(t)\n",
        "        ]\n",
        "        if uncovered:\n",
        "            if verbose:\n",
        "                print(f\"ğŸš« Gate: æ˜ç¢ºçŸ¥è­˜ç¼ºå£ â€” {uncovered}\")\n",
        "            return GateResult(\n",
        "                decision=GateDecision.REFUSE,\n",
        "                reason_code=\"EXPLICIT_KNOWLEDGE_GAP\",\n",
        "                reason_message=f\"æƒ…å¢ƒæ˜ç¢ºè¡¨ç¤ºç¼ºä¹ {', '.join(uncovered)} ç›¸é—œæ•™æï¼Œç„¡æ³•é€²è¡Œåˆ†æ\",\n",
        "                metadata={\n",
        "                    \"detected_topics\": list(detected_topics),\n",
        "                    \"uncovered_topics\": uncovered,\n",
        "                },\n",
        "            )\n",
        "\n",
        "    # ----------------------------\n",
        "    # ä¸»é¡Œåˆ†é¡\n",
        "    # ----------------------------\n",
        "    covered_topics = [\n",
        "        t for t in detected_topics if manifest.is_topic_covered(t)\n",
        "    ]\n",
        "    explicitly_not_covered = [\n",
        "        t for t in detected_topics if manifest.is_topic_explicitly_not_covered(t)\n",
        "    ]\n",
        "\n",
        "    # ----------------------------\n",
        "    # Rule 2: å®Œå…¨è¶…å‡ºç³»çµ±ç¯„åœï¼ˆçœŸçš„æ²’æ•‘ï¼‰\n",
        "    # ----------------------------\n",
        "    if explicitly_not_covered and not covered_topics:\n",
        "        if verbose:\n",
        "            print(f\"ğŸš« Gate: ä¸»é¡Œå®Œå…¨è¶…å‡ºç¯„åœ â€” {explicitly_not_covered}\")\n",
        "        return GateResult(\n",
        "            decision=GateDecision.REFUSE,\n",
        "            reason_code=\"TOPIC_NOT_COVERED\",\n",
        "            reason_message=f\"æœ¬ç³»çµ±æœªåŒ…å« {', '.join(explicitly_not_covered)} ç›¸é—œæ•™æ\",\n",
        "            metadata={\n",
        "                \"detected_topics\": list(detected_topics),\n",
        "                \"explicitly_not_covered\": explicitly_not_covered,\n",
        "            },\n",
        "        )\n",
        "\n",
        "    # ----------------------------\n",
        "    # Rule 3: è­‰æ“šä¸è¶³ â†’ é¢¨éšªæ¨™è¨˜ï¼ˆä¸ç›´æ¥æ‹’çµ•ï¼‰\n",
        "    # ----------------------------\n",
        "    evidence_warnings = []\n",
        "    present_evidence = detector.detect_evidence_fields(scenario)\n",
        "\n",
        "    for topic in detected_topics:\n",
        "        required = manifest.get_required_evidence(topic)\n",
        "        if not required:\n",
        "            continue\n",
        "\n",
        "        missing = set(required) - present_evidence\n",
        "        missing_ratio = len(missing) / len(required)\n",
        "\n",
        "        if missing_ratio > 0:\n",
        "            evidence_warnings.append({\n",
        "                \"topic\": topic,\n",
        "                \"missing_evidence\": list(missing),\n",
        "                \"missing_ratio\": round(missing_ratio, 2),\n",
        "                \"severity\": (\n",
        "                    \"high\" if missing_ratio > 0.5\n",
        "                    else \"medium\" if missing_ratio > 0.25\n",
        "                    else \"low\"\n",
        "                )\n",
        "            })\n",
        "\n",
        "    if verbose and evidence_warnings:\n",
        "        print(f\"âš ï¸ Gate Warning: è­‰æ“šä¸è¶³æ¨™è¨˜ â€” {evidence_warnings}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # ALLOWï¼ˆé™„å¸¶é¢¨éšªåœ°åœ–ï¼‰\n",
        "    # ----------------------------\n",
        "    if verbose:\n",
        "        print(f\"âœ… Gate: ALLOW â€” Covered topics: {covered_topics}\")\n",
        "\n",
        "    return GateResult(\n",
        "        decision=GateDecision.ALLOW,\n",
        "        metadata={\n",
        "            \"detected_topics\": list(detected_topics),\n",
        "            \"covered_topics\": covered_topics,\n",
        "            \"explicitly_not_covered\": explicitly_not_covered,\n",
        "            \"evidence_warnings\": evidence_warnings,\n",
        "            \"gate_mode\": \"balanced_v3\"\n",
        "        },\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kysi7tAO6lB2"
      },
      "outputs": [],
      "source": [
        "# old version\n",
        "# def pre_llm_gate(\n",
        "#     scenario: str,\n",
        "#     manifest: KnowledgeManifest = None,\n",
        "#     detector: TopicDetector = None,\n",
        "#     verbose: bool = False,\n",
        "# ) -> GateResult:\n",
        "#     \"\"\"\n",
        "#     Pre-LLM Gateï¼šåœ¨ LLM åˆ†æå‰é€²è¡Œ deterministic åˆ¤æ–·\n",
        "\n",
        "#     Rules:\n",
        "#         1. æ˜ç¢ºçŸ¥è­˜ç¼ºå£ â†’ REFUSE\n",
        "#         2. ä¸»é¡Œå®Œå…¨è¶…å‡ºç¯„åœ â†’ REFUSE\n",
        "#         3. è­‰æ“šä¸è¶³ï¼ˆç‰¹å®šä¸»é¡Œï¼‰â†’ REFUSE\n",
        "#         4. å…¶ä»– â†’ ALLOW\n",
        "\n",
        "#     Args:\n",
        "#         scenario: æƒ…å¢ƒæè¿°\n",
        "#         manifest: çŸ¥è­˜æ¸…å–®ï¼ˆNone æ™‚ä½¿ç”¨é è¨­ï¼‰\n",
        "#         detector: ä¸»é¡Œåµæ¸¬å™¨ï¼ˆNone æ™‚ä½¿ç”¨é è¨­ï¼‰\n",
        "#         verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°éç¨‹\n",
        "\n",
        "#     Returns:\n",
        "#         GateResult: åŒ…å« decision, reason_code, reason_message, metadata\n",
        "#     \"\"\"\n",
        "#     if manifest is None:\n",
        "#         manifest = KnowledgeManifest()\n",
        "#     if detector is None:\n",
        "#         detector = TopicDetector()\n",
        "\n",
        "#     # Rule 1: æ˜ç¢ºçŸ¥è­˜ç¼ºå£\n",
        "#     if detector.detect_explicit_knowledge_gap(scenario):\n",
        "#         detected_topics = detector.detect_topics(scenario)\n",
        "#         uncovered = [\n",
        "#             t for t in detected_topics\n",
        "#             if manifest.is_topic_explicitly_not_covered(t)\n",
        "#         ]\n",
        "\n",
        "#         if uncovered:\n",
        "#             if verbose:\n",
        "#                 print(f\"ğŸš« Gate: æ˜ç¢ºçŸ¥è­˜ç¼ºå£ â€” {uncovered}\")\n",
        "\n",
        "#             return GateResult(\n",
        "#                 decision=GateDecision.REFUSE,\n",
        "#                 reason_code=\"EXPLICIT_KNOWLEDGE_GAP\",\n",
        "#                 reason_message=f\"æƒ…å¢ƒæ˜ç¢ºè¡¨ç¤ºç¼ºä¹ {', '.join(uncovered)} ç›¸é—œæ•™æï¼Œç„¡æ³•é€²è¡Œåˆ†æ\",\n",
        "#                 metadata={\n",
        "#                     \"detected_topics\": list(detected_topics),\n",
        "#                     \"uncovered_topics\": uncovered,\n",
        "#                 },\n",
        "#             )\n",
        "\n",
        "#     # Rule 2: ä¸»é¡Œè¶…å‡ºç¯„åœ\n",
        "#     detected_topics = detector.detect_topics(scenario)\n",
        "#     explicitly_not_covered = [\n",
        "#         t for t in detected_topics\n",
        "#         if manifest.is_topic_explicitly_not_covered(t)\n",
        "#     ]\n",
        "#     covered_topics_detected = [\n",
        "#         t for t in detected_topics\n",
        "#         if manifest.is_topic_covered(t)\n",
        "#     ]\n",
        "\n",
        "#     if explicitly_not_covered and not covered_topics_detected:\n",
        "#         if verbose:\n",
        "#             print(f\"ğŸš« Gate: ä¸»é¡Œè¶…å‡ºç¯„åœ â€” {explicitly_not_covered}\")\n",
        "\n",
        "#         return GateResult(\n",
        "#             decision=GateDecision.REFUSE,\n",
        "#             reason_code=\"TOPIC_NOT_COVERED\",\n",
        "#             reason_message=f\"æœ¬ç³»çµ±æœªåŒ…å« {', '.join(explicitly_not_covered)} ç›¸é—œæ•™æ\",\n",
        "#             metadata={\n",
        "#                 \"detected_topics\": list(detected_topics),\n",
        "#                 \"explicitly_not_covered\": explicitly_not_covered,\n",
        "#             },\n",
        "#         )\n",
        "\n",
        "#     # Rule 3: è­‰æ“šä¸è¶³\n",
        "#     for topic in detected_topics:\n",
        "#         required = manifest.get_required_evidence(topic)\n",
        "#         if required:\n",
        "#             present_evidence = detector.detect_evidence_fields(scenario)\n",
        "#             missing = set(required) - present_evidence\n",
        "\n",
        "#             if len(missing) > len(required) / 2:\n",
        "#                 if verbose:\n",
        "#                     print(f\"ğŸš« Gate: è­‰æ“šä¸è¶³ â€” {topic}\")\n",
        "\n",
        "#                 return GateResult(\n",
        "#                     decision=GateDecision.REFUSE,\n",
        "#                     reason_code=\"INSUFFICIENT_EVIDENCE\",\n",
        "#                     reason_message=f\"{topic} åˆ†æéœ€è¦æ›´å¤šè­‰æ“šè³‡è¨Š\",\n",
        "#                     metadata={\n",
        "#                         \"topic\": topic,\n",
        "#                         \"missing_evidence\": list(missing),\n",
        "#                     },\n",
        "#                 )\n",
        "\n",
        "#     if verbose:\n",
        "#         print(f\"âœ… Gate: ALLOW â€” {detected_topics}\")\n",
        "\n",
        "#     return GateResult(\n",
        "#         decision=GateDecision.ALLOW,\n",
        "#         metadata={\"detected_topics\": list(detected_topics)},\n",
        "#     )\n",
        "\n",
        "\n",
        "# print(\"âœ… PART 1: Pre-LLM Gate æ¨¡çµ„å·²è¼‰å…¥\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGZx1JpjfWnU"
      },
      "source": [
        "# ğŸ” PART 2: æª¢ç´¢æ¨¡çµ„ (Retrieval Pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlPu4WwjoNEz"
      },
      "source": [
        "è·è²¬ï¼šRead-Only\n",
        "- è¼‰å…¥å·²å»ºç«‹çš„ç´¢å¼•\n",
        "- æä¾› Dense Searchï¼ˆå‘é‡æª¢ç´¢ï¼‰\n",
        "- æä¾› BM25 Searchï¼ˆé—œéµå­—æª¢ç´¢ï¼‰\n",
        "- æä¾› Hybrid Searchï¼ˆæ··åˆæª¢ç´¢ï¼‰\n",
        "\n",
        "åŸ·è¡Œæ™‚æ©Ÿï¼š\n",
        "- æ¯æ¬¡æŸ¥è©¢æ™‚\n",
        "- ä¸éœ€è¦é‡å»ºç´¢å¼•ï¼Œåªéœ€è¼‰å…¥\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq6U6AvzPlje"
      },
      "source": [
        "### 2.0 è¼‰å…¥ç´¢å¼•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSsqoKms7QYE"
      },
      "outputs": [],
      "source": [
        "def load_all_indexes(index_dir: str) -> dict:\n",
        "    \"\"\"\n",
        "    è¼‰å…¥æ‰€æœ‰ç´¢å¼•å’Œè³‡æ–™\n",
        "\n",
        "    Args:\n",
        "        index_dir: ç´¢å¼•å­˜æ”¾è·¯å¾‘\n",
        "\n",
        "    Returns:\n",
        "        dict: faiss_index, chunks, bm25_index, tokenized_corpus,\n",
        "              embedding_model, metadata\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: å¦‚æœç´¢å¼•æª”æ¡ˆä¸å­˜åœ¨\n",
        "        ValueError: å¦‚æœ FAISS å‘é‡æ•¸èˆ‡ chunks æ•¸ä¸ä¸€è‡´\n",
        "    \"\"\"\n",
        "    base_path = Path(index_dir)\n",
        "\n",
        "    required_files = [\n",
        "        \"faiss_index.bin\", \"chunks.json\",\n",
        "        \"bm25_index.pkl\", \"tokenized_corpus.pkl\", \"metadata.json\",\n",
        "    ]\n",
        "\n",
        "    missing_files = [f for f in required_files if not (base_path / f).exists()]\n",
        "    if missing_files:\n",
        "        raise FileNotFoundError(\n",
        "            f\"ç¼ºå°‘ç´¢å¼•æª”æ¡ˆ: {missing_files}\\n\"\n",
        "            f\"è«‹å…ˆåŸ·è¡Œ build_data notebook å»ºç«‹ç´¢å¼•ã€‚\"\n",
        "        )\n",
        "\n",
        "    print(f\"ğŸ“‚ è¼‰å…¥ç´¢å¼•å¾: {index_dir}\")\n",
        "\n",
        "    # 1. Metadataï¼ˆå…ˆè¼‰å…¥ä»¥ç²å–æ¨¡å‹åç¨±ï¼‰\n",
        "    with open(base_path / \"metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        metadata = json.load(f)\n",
        "    print(f\"   âœ… metadata.json (version: {metadata.get('version', 'unknown')})\")\n",
        "    print(f\"      å»ºç«‹æ™‚é–“: {metadata.get('created_at', 'unknown')}\")\n",
        "    print(f\"      Chunk Size: {metadata.get('config', {}).get('chunk_size', 'unknown')}\")\n",
        "\n",
        "    # 2. FAISS Index\n",
        "    faiss_index = faiss.read_index(str(base_path / \"faiss_index.bin\"))\n",
        "    print(f\"   âœ… faiss_index.bin ({faiss_index.ntotal} vectors, dim={faiss_index.d})\")\n",
        "\n",
        "    # 3. Chunks\n",
        "    with open(base_path / \"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        chunks = json.load(f)\n",
        "    print(f\"   âœ… chunks.json ({len(chunks)} chunks)\")\n",
        "\n",
        "    # 4. BM25 Index\n",
        "    with open(base_path / \"bm25_index.pkl\", \"rb\") as f:\n",
        "        bm25_index = pickle.load(f)\n",
        "    print(f\"   âœ… bm25_index.pkl\")\n",
        "\n",
        "    # 5. Tokenized Corpus\n",
        "    with open(base_path / \"tokenized_corpus.pkl\", \"rb\") as f:\n",
        "        tokenized_corpus = pickle.load(f)\n",
        "    print(f\"   âœ… tokenized_corpus.pkl\")\n",
        "\n",
        "    # 6. Embedding Modelï¼ˆå¾ metadata è®€å–æ¨¡å‹åç¨±ï¼‰\n",
        "    # [BUG FIX] åŸæœ¬è®€ metadata.get(\"embedding_model_name\")\n",
        "    # ä½† build_data å¯«å…¥çš„ key æ˜¯ config.embedding_model\n",
        "    embedding_model_name = metadata.get(\"config\", {}).get(\n",
        "        \"embedding_model\", EMBEDDING_MODEL_NAME\n",
        "    )\n",
        "    embedding_model = SentenceTransformer(embedding_model_name)\n",
        "    print(f\"   âœ… embedding_model ({embedding_model_name})\")\n",
        "\n",
        "    # é©—è­‰ä¸€è‡´æ€§\n",
        "    assert faiss_index.ntotal == len(chunks), (\n",
        "        f\"âŒ è³‡æ–™ä¸ä¸€è‡´ï¼šFAISS æœ‰ {faiss_index.ntotal} å€‹å‘é‡ï¼Œ\"\n",
        "        f\"ä½† chunks æœ‰ {len(chunks)} å€‹\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"faiss_index\": faiss_index,\n",
        "        \"chunks\": chunks,\n",
        "        \"bm25_index\": bm25_index,\n",
        "        \"tokenized_corpus\": tokenized_corpus,\n",
        "        \"embedding_model\": embedding_model,\n",
        "        \"metadata\": metadata,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKYaAlsoioPs"
      },
      "outputs": [],
      "source": [
        "# === åŸ·è¡Œè¼‰å…¥ ===\n",
        "print(\"=\" * 60)\n",
        "print(\"ğŸ” PART 2: æª¢ç´¢æ¨¡çµ„\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "loaded = load_all_indexes(INDEX_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBUi6tPt7X9s"
      },
      "outputs": [],
      "source": [
        "# è§£åŒ…ç‚ºæ¨¡çµ„ç´šè®Šæ•¸\n",
        "faiss_index = loaded[\"faiss_index\"]\n",
        "chunks = loaded[\"chunks\"]\n",
        "bm25_index = loaded[\"bm25_index\"]\n",
        "tokenized_corpus = loaded[\"tokenized_corpus\"]\n",
        "embedding_model = loaded[\"embedding_model\"]  # SentenceTransformerï¼Œç”¨æ–¼å‘é‡æª¢ç´¢\n",
        "index_metadata = loaded[\"metadata\"]          # â† æ”¹åé¿å…è·Ÿå…¶ä»– metadata æ··æ·†\n",
        "\n",
        "print(f\"\\nâœ… ç´¢å¼•è¼‰å…¥å®Œæˆ\")\n",
        "print(f\"ğŸ“¦ ç´¢å¼•ç‰ˆæœ¬: {index_metadata.get('version')} | chunks: {len(chunks)} | vectors: {faiss_index.ntotal}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo22FdIOfeyd"
      },
      "source": [
        "### 2.1 Dense Searchï¼ˆå‘é‡æœå°‹ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1B8X3HN7qyG"
      },
      "outputs": [],
      "source": [
        "def dense_search(\n",
        "    query: str,\n",
        "    faiss_index: faiss.Index,\n",
        "    chunks: list,\n",
        "    embedding_model: SentenceTransformer,\n",
        "    k: int = DEFAULT_TOP_K,\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦é€²è¡Œæœå°‹\n",
        "\n",
        "    Args:\n",
        "        query: æŸ¥è©¢æ–‡å­—\n",
        "        faiss_index: FAISS å‘é‡ç´¢å¼•\n",
        "        chunks: chunk åˆ—è¡¨\n",
        "        embedding_model: SentenceTransformer æ¨¡å‹\n",
        "        k: è¿”å›çš„çµæœæ•¸é‡\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: æ¯å€‹ dict åŒ…å« {\"chunk\": ChunkDict, \"score\": float}\n",
        "    \"\"\"\n",
        "    query_embedding = embedding_model.encode(\n",
        "        [query], normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "    scores, indices = faiss_index.search(\n",
        "        np.array(query_embedding, dtype=\"float32\"), k\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "    for idx, score in zip(indices[0], scores[0]):\n",
        "        if idx < len(chunks):  # é˜²ç¦¦æ€§æª¢æŸ¥\n",
        "            results.append({\n",
        "                \"chunk\": chunks[idx],\n",
        "                \"score\": float(score),\n",
        "            })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEaMoM4GfkOi"
      },
      "source": [
        "### 2.2 BM25 Searchï¼ˆé—œéµå­—æœå°‹ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4rQDMXU77UI"
      },
      "outputs": [],
      "source": [
        "def bm25_search(\n",
        "    query: str,\n",
        "    bm25_index,\n",
        "    tokenized_corpus: list,\n",
        "    chunks: list,\n",
        "    k: int = DEFAULT_TOP_K,\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨ BM25 é€²è¡Œé—œéµå­—æœå°‹\n",
        "\n",
        "    Args:\n",
        "        query: æŸ¥è©¢æ–‡å­—\n",
        "        bm25_index: BM25Okapi ç´¢å¼•\n",
        "        tokenized_corpus: åˆ†è©å¾Œçš„èªæ–™\n",
        "        chunks: chunk åˆ—è¡¨\n",
        "        k: è¿”å›çš„çµæœæ•¸é‡\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: æ¯å€‹ dict åŒ…å« {\"chunk\": ChunkDict, \"score\": float}\n",
        "    \"\"\"\n",
        "    # åˆ¤æ–·æŸ¥è©¢èªè¨€ä¸¦åˆ†è©\n",
        "    if any(\"\\u4e00\" <= char <= \"\\u9fff\" for char in query):\n",
        "        query_tokens = list(jieba.cut(query))\n",
        "    else:\n",
        "        query_tokens = query.lower().split()\n",
        "\n",
        "    scores = bm25_index.get_scores(query_tokens)\n",
        "    top_k_indices = np.argsort(scores)[::-1][:k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_k_indices:\n",
        "        results.append({\n",
        "            \"chunk\": chunks[idx],\n",
        "            \"score\": float(scores[idx]),\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBLJRgOzfs_y"
      },
      "source": [
        "### 2.3 Hybrid Searchï¼ˆæ··åˆæœå°‹ + RRFï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JHy9igJ8C1Y"
      },
      "outputs": [],
      "source": [
        "def hybrid_search(\n",
        "    query: str,\n",
        "    faiss_index: faiss.Index,\n",
        "    chunks: list,\n",
        "    embedding_model: SentenceTransformer,\n",
        "    bm25_index,\n",
        "    tokenized_corpus: list,\n",
        "    k: int = DEFAULT_TOP_K,\n",
        "    rrf_k: int = DEFAULT_RRF_K,\n",
        "    use_priority_weighting: bool = DEFAULT_USE_PRIORITY_WEIGHTING,\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    æ··åˆæœå°‹ï¼šDense + BM25ï¼Œä½¿ç”¨ RRF (Reciprocal Rank Fusion) èåˆæ’å\n",
        "\n",
        "    Args:\n",
        "        query: æŸ¥è©¢æ–‡å­—\n",
        "        faiss_index: FAISS å‘é‡ç´¢å¼•\n",
        "        chunks: chunk åˆ—è¡¨\n",
        "        embedding_model: SentenceTransformer æ¨¡å‹\n",
        "        bm25_index: BM25 ç´¢å¼•\n",
        "        tokenized_corpus: åˆ†è©å¾Œçš„èªæ–™\n",
        "        k: è¿”å›çš„çµæœæ•¸é‡\n",
        "        rrf_k: RRF åƒæ•¸ï¼ˆé€šå¸¸ç‚º 60ï¼‰\n",
        "        use_priority_weighting: æ˜¯å¦ä½¿ç”¨ retrieval_priority åŠ æ¬Š\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: æ¯å€‹ dict åŒ…å« chunk, score, raw_rrf_score, priority_weight\n",
        "\n",
        "    History:\n",
        "        - v1: åŸºç¤ RRF\n",
        "        - v2: åŠ å…¥ priority_weightingï¼ˆç”¨ metadata çš„ retrieval_priority åŠ æ¬Šï¼‰\n",
        "    \"\"\"\n",
        "    # å–å¾—å…©ç¨®æœå°‹çµæœï¼ˆå¤šå–ä»¥ç¢ºä¿èåˆæ•ˆæœï¼‰\n",
        "    dense_results = dense_search(\n",
        "        query, faiss_index, chunks, embedding_model, k=k * 2\n",
        "    )\n",
        "    bm25_results = bm25_search(\n",
        "        query, bm25_index, tokenized_corpus, chunks, k=k * 2\n",
        "    )\n",
        "\n",
        "    # å»ºç«‹ chunk_id â†’ æ’å\n",
        "    dense_ranks = {}\n",
        "    for rank, r in enumerate(dense_results, start=1):\n",
        "        chunk_id = r[\"chunk\"][\"chunk_id\"]\n",
        "        dense_ranks[chunk_id] = rank\n",
        "\n",
        "    bm25_ranks = {}\n",
        "    for rank, r in enumerate(bm25_results, start=1):\n",
        "        chunk_id = r[\"chunk\"][\"chunk_id\"]\n",
        "        bm25_ranks[chunk_id] = rank\n",
        "\n",
        "    # RRF èåˆ\n",
        "    all_chunk_ids = set(dense_ranks.keys()) | set(bm25_ranks.keys())\n",
        "    rrf_scores = {}\n",
        "\n",
        "    for chunk_id in all_chunk_ids:\n",
        "        score = 0\n",
        "        if chunk_id in dense_ranks:\n",
        "            score += 1 / (rrf_k + dense_ranks[chunk_id])\n",
        "        if chunk_id in bm25_ranks:\n",
        "            score += 1 / (rrf_k + bm25_ranks[chunk_id])\n",
        "        rrf_scores[chunk_id] = score\n",
        "\n",
        "    # æ¬Šé‡èª¿æ•´\n",
        "    chunk_lookup = {c[\"chunk_id\"]: c for c in chunks}\n",
        "\n",
        "    if use_priority_weighting:\n",
        "        weighted_scores = {}\n",
        "        for chunk_id, rrf_score in rrf_scores.items():\n",
        "            chunk = chunk_lookup[chunk_id]\n",
        "            priority = chunk.get(\"retrieval_priority\", 1.0)\n",
        "            weighted_scores[chunk_id] = rrf_score * priority\n",
        "    else:\n",
        "        weighted_scores = rrf_scores\n",
        "\n",
        "    # æ’åºå– top-k\n",
        "    sorted_chunk_ids = sorted(\n",
        "        weighted_scores.keys(),\n",
        "        key=lambda x: weighted_scores[x],\n",
        "        reverse=True,\n",
        "    )[:k]\n",
        "\n",
        "    # çµ„è£çµæœ\n",
        "    results = []\n",
        "    for chunk_id in sorted_chunk_ids:\n",
        "        results.append({\n",
        "            \"chunk\": chunk_lookup[chunk_id],\n",
        "            \"score\": weighted_scores[chunk_id],\n",
        "            \"raw_rrf_score\": rrf_scores[chunk_id],\n",
        "            \"priority_weight\": chunk_lookup[chunk_id].get(\"retrieval_priority\", 1.0),\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOOfc1XdTyfQ"
      },
      "source": [
        "### 2.4 ä¾¿æ·å‡½æ•¸ retrieve_contexts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2bKTyBMg6Zh"
      },
      "source": [
        "å–å¾—ä¸Šä¸‹æ–‡ï¼ˆä¾› PART 3 ä½¿ç”¨ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNx_5PeOT2kO"
      },
      "outputs": [],
      "source": [
        "def retrieve_contexts(\n",
        "    query: str,\n",
        "    faiss_index: faiss.Index,\n",
        "    chunks: list,\n",
        "    embedding_model: SentenceTransformer,\n",
        "    bm25_index,\n",
        "    tokenized_corpus: list,\n",
        "    k: int = DEFAULT_TOP_K,\n",
        "    method: str = \"hybrid\",\n",
        "    use_priority_weighting: bool = DEFAULT_USE_PRIORITY_WEIGHTING,\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    æª¢ç´¢ç›¸é—œä¸Šä¸‹æ–‡ï¼Œä¾› LLM åˆ†æä½¿ç”¨ã€‚\n",
        "    é€™æ˜¯ PART 2 æä¾›çµ¦ PART 3 çš„ä¸»è¦ä»‹é¢ã€‚\n",
        "\n",
        "    Args:\n",
        "        query: æŸ¥è©¢æ–‡å­—ï¼ˆé€šå¸¸æ˜¯æƒ…å¢ƒæè¿°ï¼‰\n",
        "        faiss_index: FAISS å‘é‡ç´¢å¼•\n",
        "        chunks: chunk åˆ—è¡¨\n",
        "        embedding_model: SentenceTransformer æ¨¡å‹\n",
        "        bm25_index: BM25 ç´¢å¼•\n",
        "        tokenized_corpus: åˆ†è©å¾Œçš„èªæ–™\n",
        "        k: è¿”å›çš„çµæœæ•¸é‡\n",
        "        method: æª¢ç´¢æ–¹æ³• (\"dense\", \"bm25\", \"hybrid\")\n",
        "        use_priority_weighting: æ˜¯å¦ä½¿ç”¨ retrieval_priority åŠ æ¬Š\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡åˆ—è¡¨\n",
        "    \"\"\"\n",
        "    if method == \"dense\":\n",
        "        search_results = dense_search(\n",
        "            query, faiss_index, chunks, embedding_model, k\n",
        "        )\n",
        "    elif method == \"bm25\":\n",
        "        search_results = bm25_search(\n",
        "            query, bm25_index, tokenized_corpus, chunks, k\n",
        "        )\n",
        "    else:  # hybridï¼ˆé è¨­ï¼‰\n",
        "        search_results = hybrid_search(\n",
        "            query, faiss_index, chunks, embedding_model,\n",
        "            bm25_index, tokenized_corpus, k,\n",
        "            use_priority_weighting=use_priority_weighting,  # â† [BUG FIX] åŸæœ¬æ¼å‚³\n",
        "        )\n",
        "\n",
        "    # è½‰æ›ç‚º ContextDict æ ¼å¼\n",
        "    contexts = []\n",
        "    for r in search_results:\n",
        "        chunk = r[\"chunk\"]\n",
        "        contexts.append({\n",
        "            \"chunk_id\": chunk.get(\"chunk_id\", \"\"),\n",
        "            \"source\": chunk.get(\"source\", \"Unknown\"),\n",
        "            \"page\": chunk.get(\"page\", 0),\n",
        "            \"text\": chunk.get(\"text\", \"\"),\n",
        "            \"score\": r[\"score\"],\n",
        "            \"doc_category\": chunk.get(\"doc_category\", \"unknown\"),\n",
        "            \"explanation_style\": chunk.get(\"explanation_style\", \"neutral\"),\n",
        "        })\n",
        "\n",
        "    return contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_SWzQllf4KP"
      },
      "source": [
        "### 2.5 ã€æ¸¬è©¦ã€‘å¿«é€Ÿé©—è­‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ddDI7WkooZd"
      },
      "outputs": [],
      "source": [
        "# æ¸¬è©¦æŸ¥è©¢  # è³‡æ–™ç§‘å­¸/å¯¦é©—è€… thinking\n",
        "test_query = \"ä»€éº¼æ˜¯æ´—éŒ¢ï¼Ÿ\"\n",
        "\n",
        "print(f\"\\nğŸ§ª æ¸¬è©¦æŸ¥è©¢: {test_query}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Dense Search\n",
        "print(\"\\nğŸ“Š Dense Search çµæœ:\")\n",
        "dense_results = dense_search(\n",
        "    test_query, faiss_index, chunks, embedding_model, k=3\n",
        ")\n",
        "for i, r in enumerate(dense_results, 1):\n",
        "    print(f\"   [{i}] {r['chunk']['source']} p.{r['chunk']['page']} \"\n",
        "          f\"(score: {r['score']:.3f})\")\n",
        "\n",
        "# BM25 Search\n",
        "print(\"\\nğŸ“Š BM25 Search çµæœ:\")\n",
        "bm25_results = bm25_search(\n",
        "    test_query, bm25_index, tokenized_corpus, chunks, k=3\n",
        ")\n",
        "for i, r in enumerate(bm25_results, 1):\n",
        "    print(f\"   [{i}] {r['chunk']['source']} p.{r['chunk']['page']} \"\n",
        "          f\"(score: {r['score']:.3f})\")\n",
        "\n",
        "# Hybrid Search\n",
        "print(\"\\nğŸ“Š Hybrid Search çµæœ:\")\n",
        "hybrid_results = hybrid_search(\n",
        "    test_query, faiss_index, chunks, embedding_model,\n",
        "    bm25_index, tokenized_corpus, k=3\n",
        ")\n",
        "for i, r in enumerate(hybrid_results, 1):\n",
        "    print(f\"   [{i}] {r['chunk']['source']} p.{r['chunk']['page']} \"\n",
        "          f\"(score: {r['score']:.3f})\")\n",
        "\n",
        "print(f\"   [{i}] {r['chunk']['source']} p.{r['chunk']['page']} \"\n",
        "      f\"(Score: {r['score']:.3f} | Raw: {r['raw_rrf_score']:.3f} | W: {r['priority_weight']})\")\n",
        "print(\"\\nâœ… PART 2 æ¸¬è©¦å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrMuUxvtUQZx"
      },
      "outputs": [],
      "source": [
        "# # é–‹ç™¼è€…thinking\n",
        "# if __name__ == \"__main__\":\n",
        "#     # æ¸¬è©¦æŸ¥è©¢\n",
        "#     test_query = \"ä»€éº¼æ˜¯æ´—éŒ¢ï¼Ÿ\"\n",
        "\n",
        "#     print(f\"\\nğŸ§ª æ¸¬è©¦æŸ¥è©¢: {test_query}\")\n",
        "#     print(\"-\" * 40)\n",
        "\n",
        "#     # Dense Search\n",
        "#     print(\"\\nğŸ“Š Dense Search çµæœ:\")\n",
        "#     dense_results = dense_search(\n",
        "#         test_query, faiss_index, chunks, embedding_model, k=3\n",
        "#     )\n",
        "#     for i, r in enumerate(dense_results, 1):\n",
        "#         print(f\"   [{i}] {r['chunk']['source']} p.{r['chunk']['page']} \"\n",
        "#               f\"(score: {r['score']:.3f})\")\n",
        "\n",
        "#     # BM25 Search\n",
        "#     print(\"\\nğŸ“Š BM25 Search çµæœ:\")\n",
        "#     bm25_results = bm25_search(\n",
        "#         test_query, bm25_index, tokenized_corpus, chunks, k=3\n",
        "#     )\n",
        "#     for i, r in enumerate(bm25_results, 1):\n",
        "#         print(f\"   [{i}] {r['chunk']['source']} p.{r['chunk']['page']} \"\n",
        "#               f\"(score: {r['score']:.3f})\")\n",
        "\n",
        "#     # Hybrid Search\n",
        "#     print(\"\\nğŸ“Š Hybrid Search çµæœ:\")\n",
        "#     hybrid_results = hybrid_search(\n",
        "#         test_query, faiss_index, chunks, embedding_model,\n",
        "#         bm25_index, tokenized_corpus, k=3\n",
        "#     )\n",
        "#     for i, r in enumerate(hybrid_results, 1):\n",
        "#         print(f\"   [{i}] {r['chunk']['source']} p.{r['chunk']['page']} \"\n",
        "#               f\"(score: {r['score']:.3f})\")\n",
        "\n",
        "#     print(\"\\nâœ… PART 2 æ¸¬è©¦å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja7-J-Uxf7-o"
      },
      "source": [
        "é€™ 12 é¡Œæ˜¯æ¸¬è©¦ã€Œæª¢ç´¢ã€åŠŸèƒ½çš„â€”â€”èƒ½ä¸èƒ½æ‰¾åˆ°æ­£ç¢ºçš„é é¢ï¼Ÿ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znWOxcixfz7v"
      },
      "outputs": [],
      "source": [
        "RETRIEVAL_TEST_CASES = [\n",
        "    # Easy\n",
        "    {\"id\": \"E1\", \"query\": \"ä»€éº¼æ˜¯æ´—éŒ¢ï¼Ÿ\", \"expected_page\": 2, \"difficulty\": \"easy\"},\n",
        "    {\"id\": \"E2\", \"query\": \"æ´—éŒ¢çš„ä¸‰ç¨®æ…‹æ¨£æ˜¯ä»€éº¼ï¼Ÿ\", \"expected_page\": 2, \"difficulty\": \"easy\"},\n",
        "    {\"id\": \"E3\", \"query\": \"éŠ€è¡Œç¾é‡‘äº¤æ˜“å¤šå°‘éŒ¢ä»¥ä¸Šè¦ç”³å ±ï¼Ÿ\", \"expected_page\": 8, \"difficulty\": \"easy\"},\n",
        "\n",
        "    # Medium\n",
        "    {\"id\": \"M1\", \"query\": \"æŠŠå¤§ç­†ç¾é‡‘æ‹†æˆå°ç­†å­˜å…¥éŠ€è¡Œæœƒæœ‰ä»€éº¼æ³•å¾‹å•é¡Œï¼Ÿ\", \"expected_page\": 8, \"difficulty\": \"medium\"},\n",
        "    {\"id\": \"M2\", \"query\": \"é–‹å…¬å¸æˆ¶é ­è¦å¸¶ä»€éº¼æ–‡ä»¶ï¼Ÿ\", \"expected_page\": 5, \"difficulty\": \"medium\"},\n",
        "    {\"id\": \"M3\", \"query\": \"éŠ€è¡Œä»€éº¼æƒ…æ³ä¸‹æœƒæ‹’çµ•è®“æˆ‘é–‹æˆ¶ï¼Ÿ\", \"expected_page\": 6, \"difficulty\": \"medium\"},\n",
        "    {\"id\": \"M4\", \"query\": \"ç‚ºä»€éº¼é‡‘èæ©Ÿæ§‹è¦åšå®¢æˆ¶å¯©æŸ¥ï¼Ÿ\", \"expected_page\": 4, \"difficulty\": \"medium\"},\n",
        "\n",
        "    # Hard\n",
        "    {\"id\": \"H1\", \"query\": \"å­¸ç”ŸæŠŠå¸³æˆ¶è³£çµ¦åˆ¥äººæœƒæ€æ¨£ï¼Ÿ\", \"expected_page\": 9, \"difficulty\": \"hard\"},\n",
        "    {\"id\": \"H2\", \"query\": \"å‡ºåœ‹å¯ä»¥å¸¶å¤šå°‘ç¾é‡‘ï¼Ÿè¶…éæ€éº¼è¾¦ï¼Ÿ\", \"expected_page\": 7, \"difficulty\": \"hard\"},\n",
        "    {\"id\": \"H3\", \"query\": \"é«˜é¢¨éšªåœ°å€çš„å®¢æˆ¶é–‹æˆ¶æœƒè¢«æ€éº¼å°å¾…ï¼Ÿ\", \"expected_page\": 4, \"difficulty\": \"hard\"},\n",
        "\n",
        "    # Edge caseï¼ˆæ–‡ä»¶ä¸­æ²’æœ‰çš„ï¼‰\n",
        "    {\"id\": \"X1\", \"query\": \"ç”¨æ¯”ç‰¹å¹£æ´—éŒ¢æœƒæ€æ¨£ï¼Ÿ\", \"expected_page\": None, \"difficulty\": \"edge\"},\n",
        "    {\"id\": \"X2\", \"query\": \"æ´—éŒ¢é˜²åˆ¶æ³•æ˜¯å“ªä¸€å¹´åˆ¶å®šçš„ï¼Ÿ\", \"expected_page\": None, \"difficulty\": \"edge\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so318cUSgC5z"
      },
      "outputs": [],
      "source": [
        "def evaluate_retrieval(search_func, test_cases, k=3):\n",
        "    \"\"\"è©•ä¼° retrieval æº–ç¢ºåº¦\"\"\"\n",
        "    results = {\"recall@1\": 0, \"recall@3\": 0, \"details\": []}\n",
        "    valid_cases = [tc for tc in test_cases if tc[\"expected_page\"] is not None]\n",
        "\n",
        "    for tc in valid_cases:\n",
        "        search_results = search_func(tc[\"query\"])\n",
        "        retrieved_pages = [r[\"chunk\"][\"page\"] for r in search_results[:k]]\n",
        "\n",
        "        hit_at_1 = retrieved_pages[0] == tc[\"expected_page\"] if retrieved_pages else False\n",
        "        hit_at_3 = tc[\"expected_page\"] in retrieved_pages[:3]\n",
        "\n",
        "        if hit_at_1:\n",
        "            results[\"recall@1\"] += 1\n",
        "        if hit_at_3:\n",
        "            results[\"recall@3\"] += 1\n",
        "\n",
        "        results[\"details\"].append({\n",
        "            \"id\": tc[\"id\"],\n",
        "            \"query\": tc[\"query\"],\n",
        "            \"expected\": tc[\"expected_page\"],\n",
        "            \"got\": retrieved_pages[0] if retrieved_pages else None,\n",
        "            \"hit\": \"âœ…\" if hit_at_1 else \"âŒ\"\n",
        "        })\n",
        "\n",
        "    n = len(valid_cases)\n",
        "    print(f\"Recall@1: {results['recall@1']}/{n} ({results['recall@1']/n*100:.0f}%)\")\n",
        "    print(f\"Recall@3: {results['recall@3']}/{n} ({results['recall@3']/n*100:.0f}%)\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw1xZDQ8gHLE"
      },
      "outputs": [],
      "source": [
        "# åŸ·è¡Œè©•ä¼°\n",
        "search_func = lambda q: hybrid_search(q, faiss_index, chunks, embedding_model, bm25_index, tokenized_corpus)\n",
        "evaluate_retrieval(search_func, RETRIEVAL_TEST_CASES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VncezVFOgKkk"
      },
      "source": [
        "# ğŸ¤– PART 3: LLM åˆ†ææ¨¡çµ„"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhAvXv3vgQEy"
      },
      "source": [
        "é€™å€‹éƒ¨åˆ†è² è²¬ã€ŒæŠŠæª¢ç´¢åˆ°çš„æ®µè½äº¤çµ¦ LLM åˆ†æã€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqAaklPeZQtB"
      },
      "source": [
        "è·è²¬ï¼š\n",
        "- æ¥æ”¶ PART 2 æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡\n",
        "- çµ„è£ Prompt\n",
        "- å‘¼å« LLM é€²è¡Œåˆ†æ\n",
        "- æ ¼å¼åŒ–è¼¸å‡ºçµæœ\n",
        "\n",
        "**æ¨¡å‹å€åˆ†**ï¼š\n",
        "- `embedding_model`: SentenceTransformer â†’ PART 2 å‘é‡æª¢ç´¢\n",
        "- `llm_config` / `SELECTED_CONFIG`: Gemini / Groq â†’ PART 3 æ–‡æœ¬ç”Ÿæˆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN3sifb3ZjUE"
      },
      "source": [
        "### 3.0 åˆå§‹åŒ– LLM Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxSCtaVoqsT8"
      },
      "outputs": [],
      "source": [
        "llm_client = None\n",
        "\n",
        "if SELECTED_CONFIG[\"provider\"] == \"groq\":\n",
        "    llm_client = Groq(api_key=SELECTED_CONFIG[\"api_key\"])\n",
        "    print(f\"ğŸš€ Groq å¯¦é«”å·²å°±ç·’ï¼š{SELECTED_CONFIG['llm_model_name']}\")\n",
        "\n",
        "elif SELECTED_CONFIG[\"provider\"] == \"gemini\":\n",
        "    genai.configure(api_key=SELECTED_CONFIG[\"api_key\"])\n",
        "    llm_client = genai.GenerativeModel(SELECTED_CONFIG[\"llm_model_name\"])\n",
        "    print(f\"ğŸŒŸ Gemini å¯¦é«”å·²å°±ç·’ï¼š{SELECTED_CONFIG['llm_model_name']}\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"âŒ æœªçŸ¥çš„ Provider: {SELECTED_CONFIG['provider']}\")\n",
        "\n",
        "assert llm_client is not None, \"âŒ LLM Client åˆå§‹åŒ–å¤±æ•—\"\n",
        "print(f\"âœ… LLM Client å°±ç·’\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3spM8aO96DE"
      },
      "outputs": [],
      "source": [
        "# ===== æ¨ç†å‡½æ•¸ =====\n",
        "def generate_response(prompt: str):\n",
        "    \"\"\"çµ±ä¸€æ¨ç†ä»‹é¢ï¼šæ”¯æ´ Groq èˆ‡ Gemini\"\"\"\n",
        "\n",
        "    # 1. è™•ç† Groq é‚è¼¯\n",
        "    if SELECTED_CONFIG[\"provider\"] == \"groq\":\n",
        "        # Groq ä½¿ç”¨çš„æ˜¯é¡ä¼¼ OpenAI çš„ Chat Completion æ ¼å¼\n",
        "        chat_completion = llm_client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            model=SELECTED_CONFIG[\"llm_model_name\"],\n",
        "            # é€™è£¡å¯ä»¥åŠ å…¥ temperature ç­‰åƒæ•¸\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content.strip()\n",
        "\n",
        "    # 2. è™•ç† Gemini é‚è¼¯\n",
        "    elif SELECTED_CONFIG[\"provider\"] == \"gemini\":\n",
        "        # Gemini çš„ llm_client æ˜¯ genai.GenerativeModel å¯¦ä¾‹\n",
        "        response = llm_client.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    # 3. è™•ç†èˆŠçš„æˆ–æœªå®šç¾©çš„é‚è¼¯ (å®‰å…¨ç¶²)\n",
        "    else:\n",
        "        return f\"âŒ éŒ¯èª¤ï¼šæœªæ”¯æ´çš„ Provider ({SELECTED_CONFIG['provider']})\"\n",
        "\n",
        "# ===== æ¸¬è©¦ =====\n",
        "test_prompt = \"è«‹ç°¡è¦èªªæ˜ä»€éº¼æ˜¯ RAG ç³»çµ±ï¼Ÿ\"\n",
        "print(f\"âœ… ä½¿ç”¨æ¨¡å‹ï¼š{SELECTED_CONFIG['llm_model_name']}\")\n",
        "response = generate_response(test_prompt)\n",
        "print(f\"å›æ‡‰ï¼š\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVSJ_4ujgfTA"
      },
      "source": [
        "### 3.1 System Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR-WphvPrP8P"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "ä½ æ˜¯ä¸€ä½ã€ŒAML ç´…æ——åŠ©æ•™ã€ï¼Œå°ˆé–€å”åŠ©é‡‘èå¾æ¥­äººå“¡è¾¨è­˜å¯ç–‘äº¤æ˜“æƒ…å¢ƒä¸­çš„æ´—éŒ¢é˜²åˆ¶ç´…æ——æŒ‡æ¨™ã€‚\n",
        "\n",
        "## ä½ çš„è§’è‰²\n",
        "- ä½ æ˜¯ã€Œæ•™è‚²è¼”åŠ©å·¥å…·ã€ï¼Œä¸æ˜¯åŸ·æ³•æ©Ÿé—œ\n",
        "- ä½ çš„è¼¸å‡ºæ˜¯ã€Œç´…æ——æ¸…å–® + è­‰æ“šå¼•ç”¨ã€ï¼Œä¸æ˜¯å®šç½ªåˆ¤æ±º\n",
        "- æ¯å€‹åˆ¤æ–·éƒ½è¦æœ‰æ–‡ä»¶ä¾æ“š\n",
        "\n",
        "## åˆ¤å®šæ¨™æº–ï¼ˆé‡è¦ï¼ï¼‰\n",
        "### CONFIRMEDï¼ˆç¢ºèªç´…æ——ï¼‰\n",
        "- æƒ…å¢ƒä¸­**æ˜ç¢ºæè¿°**å¤šå€‹ç´…æ——ç‰¹å¾µ\n",
        "- æª¢ç´¢åˆ°çš„æ–‡ä»¶ä¸­æœ‰**å°æ‡‰çš„å®šç¾©æˆ–æ¨™æº–**\n",
        "- ä¾‹ï¼šã€Œé€£çºŒ 3 å¤©ï¼Œæ¯æ¬¡ 49 è¬ã€+ æ–‡ä»¶æåˆ°ã€Œåˆ»æ„ä½æ–¼ç”³å ±é–€æª» 50 è¬ã€\n",
        "\n",
        "### POSSIBLEï¼ˆå¯èƒ½ç´…æ——ï¼‰\n",
        "- æƒ…å¢ƒæœ‰å¯ç–‘è·¡è±¡ï¼Œä½†**ç¼ºå°‘é—œéµç´°ç¯€**\n",
        "- ä¾‹ï¼šã€Œå¤šç­†å°é¡äº¤æ˜“ã€ä½†æ²’èªªå…·é«”é‡‘é¡ã€é »ç‡ã€æ˜¯å¦æ¥è¿‘é–€æª»\n",
        "- éœ€è¦è¿½å•æ‰èƒ½ç¢ºèª\n",
        "\n",
        "### UNLIKELYï¼ˆä¸å¤ªå¯èƒ½ï¼‰\n",
        "- æƒ…å¢ƒä¸ç¬¦åˆä»»ä½•ç´…æ——ç‰¹å¾µ\n",
        "\n",
        "### REFUSEï¼ˆæ‹’çµ•åˆ¤æ–·ï¼‰\n",
        "- æƒ…å¢ƒæ¶‰åŠçš„é ˜åŸŸä¸åœ¨ä½ çš„çŸ¥è­˜ç¯„åœ\n",
        "- ä¾‹ï¼šTBMLï¼ˆè²¿æ˜“å‹æ´—éŒ¢ï¼‰ï¼Œä½†ä½ åªæœ‰è™›æ“¬è³‡ç”¢çš„æ–‡ä»¶\n",
        "\n",
        "## ä½ èªè­˜çš„ç´…æ——é¡å‹\n",
        "| ID | åç¨± | é—œéµç‰¹å¾µ |\n",
        "|----|------|----------|\n",
        "| RF-01 | é–€æª»æ‹†åˆ† | å¤šç­†å°é¡ã€æ¥è¿‘ç”³å ±é–€æª»ã€è¦é¿ç”³å ± |\n",
        "| RF-02 | å¿«é€Ÿæµè½‰ | å…¥å¸³å³è½‰å‡ºã€å¤šå°æ‰‹æ–¹ã€éè·¯å¸³æˆ¶ |\n",
        "| RF-03 | ç¾é‡‘ç•°å¸¸ | ç¾é‡‘èˆ‡èƒŒæ™¯ä¸ç¬¦ã€çªç„¶å¤§é‡ç¾é‡‘ |\n",
        "| RF-04 | ç¬¬ä¸‰äººä»£è¾¦ | ä»–äººæ“ä½œã€äººé ­å¸³æˆ¶ |\n",
        "| RF-05 | è·¨å¢ƒé«˜é¢¨éšª | é«˜é¢¨éšªåœ°å€ã€ç„¡åˆç†è·¨å¢ƒç›®çš„ |\n",
        "| RF-06 | èˆ‡èº«åˆ†ä¸ç¬¦ | äº¤æ˜“èˆ‡è·æ¥­/æ¥­å‹™ä¸ç¬¦ |\n",
        "| RF-07 | è™›æ“¬è³‡ç”¢åŒ¿å | æ··å¹£ã€éè¨—ç®¡éŒ¢åŒ…ã€éš±ç§å¹£ |\n",
        "| RF-08 | å…¬å¸ä¸é€æ˜ | ç©ºæ®¼å…¬å¸ã€å—ç›Šäººä¸æ˜ |\n",
        "\n",
        "## è¼¸å‡ºæ ¼å¼ï¼ˆJSONï¼‰\n",
        "{\n",
        "  \"scenario_summary\": \"ä¸€å¥è©±æ‘˜è¦\",\n",
        "  \"assessment\": \"confirmed | possible | unlikely | refuse\",\n",
        "  \"identified_flags\": [\n",
        "    {\n",
        "      \"flag_id\": \"RF-XX\",\n",
        "      \"flag_name\": \"åç¨±\",\n",
        "      \"confidence\": \"high | medium | low\",\n",
        "      \"reasoning\": \"åˆ†æç†ç”±\",\n",
        "      \"evidence\": [{\"source\": \"TW_GOV\", \"page\": 8, \"quote\": \"å¼•ç”¨\"}],\n",
        "      \"missing_info\": [\"ç¼ºå°‘çš„è³‡è¨Š\"]\n",
        "    }\n",
        "  ],\n",
        "  \"follow_up_questions\": [\"è¿½å•å•é¡Œ\"],\n",
        "  \"disclaimer\": \"æœ¬åˆ†æåƒ…ä¾›æ•™è‚²åƒè€ƒï¼Œä¸æ§‹æˆæ³•å¾‹æ„è¦‹ã€‚\"\n",
        "}\n",
        "\n",
        "## åˆ¤å®šæ¨™æº–\n",
        "- confirmed: æ˜ç¢ºç´…æ——ç‰¹å¾µ + è­‰æ“šå……è¶³\n",
        "- possible: å¯ç–‘è·¡è±¡ + éœ€è¦è¿½å•\n",
        "- unlikely: ä¸ç¬¦åˆç´…æ——ç‰¹å¾µ\n",
        "- refuse: è¶…å‡ºç¯„åœ / ç„¡æ³•åˆ¤æ–·\n",
        "\"\"\"\n",
        "\n",
        "print(f\"âœ… SYSTEM_PROMPT å·²å®šç¾© ({len(SYSTEM_PROMPT)} å­—å…ƒ)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQG3h6HXgoRf"
      },
      "source": [
        "### 3.2 build_user_prompt()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVqyH2Awfinl"
      },
      "outputs": [],
      "source": [
        "def build_user_prompt(scenario: str, contexts: list) -> str:\n",
        "    \"\"\"\n",
        "    çµ„è£ user prompt\n",
        "\n",
        "    Args:\n",
        "        scenario: æƒ…å¢ƒæè¿°\n",
        "        contexts: retrieve_contexts() çš„è¼¸å‡º\n",
        "\n",
        "    Returns:\n",
        "        str: å®Œæ•´çš„ user prompt\n",
        "    \"\"\"\n",
        "    context_str = \"\"\n",
        "    for i, ctx in enumerate(contexts, 1):\n",
        "        style_tag = {\n",
        "            \"authoritative\": \"ã€æ¬Šå¨ä¾†æºã€‘\",\n",
        "            \"simplified\": \"ã€ç°¡åŒ–èªªæ˜ã€‘\",\n",
        "            \"technical\": \"ã€æŠ€è¡“ç´°ç¯€ã€‘\",\n",
        "        }.get(ctx.get(\"explanation_style\", \"neutral\"), \"\")\n",
        "\n",
        "        context_str += f\"\"\"\n",
        "### æ®µè½ {i} {style_tag}\n",
        "- ä¾†æºï¼š{ctx['source']}ï¼Œç¬¬ {ctx['page']} é \n",
        "- åˆ†é¡ï¼š{ctx.get('doc_category', 'unknown')}\n",
        "- å…§å®¹ï¼š{ctx['text']}\n",
        "---\n",
        "\"\"\"\n",
        "\n",
        "    return f\"\"\"\n",
        "## æƒ…å¢ƒæè¿°\n",
        "{scenario}\n",
        "\n",
        "## æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡ä»¶\n",
        "{context_str}\n",
        "\n",
        "è«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šï¼Œåˆ†æé€™å€‹æƒ…å¢ƒä¸­å¯èƒ½å­˜åœ¨çš„ AML ç´…æ——ã€‚\n",
        "\n",
        "### ä½¿ç”¨æŒ‡å¼•\n",
        "- å„ªå…ˆå¼•ç”¨ã€æ¬Šå¨ä¾†æºã€‘çš„å®šç¾©å’Œæ¨™æº–\n",
        "- å¯ç”¨ã€ç°¡åŒ–èªªæ˜ã€‘è¼”åŠ©è§£é‡‹\n",
        "- æ‰€æœ‰åˆ¤æ–·å¿…é ˆæœ‰æ˜ç¢ºçš„æ–‡ä»¶ä¾æ“š\n",
        "\n",
        "è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºã€‚\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7Cz-vt_l76T"
      },
      "source": [
        "### 3.3 call_llm() é¸æ“‡æ¨¡å‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5-hPz-RswCV"
      },
      "outputs": [],
      "source": [
        "def call_llm(\n",
        "    system_prompt: str,\n",
        "    user_prompt: str,\n",
        "    llm_config: dict,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    å‘¼å« LLM é€²è¡Œåˆ†æï¼ˆæ”¯æ´ Gemini, Groqï¼‰\n",
        "\n",
        "    Args:\n",
        "        system_prompt: ç³»çµ±æç¤ºè©\n",
        "        user_prompt: ä½¿ç”¨è€…æç¤ºè©\n",
        "        llm_config: LLM é…ç½® dictï¼ŒåŒ…å« provider, llm_model_name, api_key\n",
        "\n",
        "    Returns:\n",
        "        dict: LLM å›æ‡‰ï¼ˆå·²è§£æçš„ JSONï¼‰\n",
        "    \"\"\"\n",
        "    provider = llm_config[\"provider\"]\n",
        "    llm_model_name = llm_config[\"llm_model_name\"]\n",
        "    api_key = llm_config[\"api_key\"]\n",
        "\n",
        "    # === Groq ===\n",
        "    if provider == \"groq\":\n",
        "        client = Groq(api_key=api_key)\n",
        "\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=llm_model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                ],\n",
        "                temperature=0.1,\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "            )\n",
        "            response_text = completion.choices[0].message.content\n",
        "            return json.loads(response_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Groq API éŒ¯èª¤: {e}\")\n",
        "            return {\n",
        "                \"assessment\": \"error\",\n",
        "                \"scenario_summary\": f\"Groq è™•ç†å¤±æ•—: {str(e)}\",\n",
        "                \"identified_flags\": [],\n",
        "                \"follow_up_questions\": [],\n",
        "                \"disclaimer\": \"ç³»çµ±éŒ¯èª¤\",\n",
        "            }\n",
        "\n",
        "    # === Gemini ===\n",
        "    elif provider == \"gemini\":\n",
        "        genai.configure(api_key=api_key)\n",
        "        model = genai.GenerativeModel(\n",
        "            model_name=llm_model_name,\n",
        "            system_instruction=system_prompt,\n",
        "            generation_config={\"response_mime_type\": \"application/json\"},\n",
        "        )\n",
        "        try:\n",
        "            response = model.generate_content(user_prompt)\n",
        "            return json.loads(response.text)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Gemini API éŒ¯èª¤: {e}\")\n",
        "            return {\n",
        "                \"assessment\": \"error\",\n",
        "                \"scenario_summary\": f\"Gemini è™•ç†å¤±æ•—: {str(e)}\",\n",
        "                \"identified_flags\": [],\n",
        "                \"follow_up_questions\": [],\n",
        "                \"disclaimer\": \"ç³»çµ±éŒ¯èª¤\",\n",
        "            }\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"ä¸æ”¯æ´çš„ LLM provider: {provider}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ9r0bEpgzTI"
      },
      "source": [
        "### 3.4 analyze_scenario()ï¼ˆä¸»å‡½æ•¸ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1_UXvU7oVMX"
      },
      "outputs": [],
      "source": [
        "def analyze_scenario(\n",
        "    scenario: str,\n",
        "    faiss_index,\n",
        "    chunks: list,\n",
        "    embedding_model,\n",
        "    bm25_index,\n",
        "    tokenized_corpus: list,\n",
        "    llm_config: dict = None,\n",
        "    k: int = DEFAULT_TOP_K,\n",
        "    retrieval_method: str = \"hybrid\",\n",
        "    use_priority_weighting: bool = DEFAULT_USE_PRIORITY_WEIGHTING,\n",
        "    enable_gate: bool = DEFAULT_ENABLE_GATE,\n",
        "    verbose: bool = True,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    ä¸»å‡½æ•¸ï¼šåˆ†æä¸€å€‹ AML æƒ…å¢ƒ\n",
        "\n",
        "    ä¸²æ¥æ‰€æœ‰æ­¥é©Ÿï¼š\n",
        "    0. (å¯é¸) Pre-LLM Gate æª¢æŸ¥\n",
        "    1. æª¢ç´¢ç›¸é—œæ®µè½\n",
        "    2. çµ„è£ Prompt\n",
        "    3. å‘¼å« LLM é€²è¡Œåˆ†æ\n",
        "\n",
        "    Args:\n",
        "        scenario: æƒ…å¢ƒæè¿°\n",
        "        llm_config: LLM é…ç½®ï¼ˆNone æ™‚ä½¿ç”¨å…¨åŸŸ SELECTED_CONFIGï¼‰\n",
        "        use_priority_weighting: æ˜¯å¦ä½¿ç”¨ retrieval_priority åŠ æ¬Šï¼ˆåŸ v2 åŠŸèƒ½ï¼‰\n",
        "        enable_gate: æ˜¯å¦å•Ÿç”¨ Pre-LLM Gateï¼ˆåŸ v3 åŠŸèƒ½ï¼‰\n",
        "\n",
        "    Returns:\n",
        "        dict: LLM åˆ†æçµæœï¼ˆJSON æ ¼å¼ï¼‰\n",
        "\n",
        "    History:\n",
        "        - v1 (2025-01): åŸºç¤ç‰ˆæœ¬\n",
        "        - v2 (2025-02-09): åŠ å…¥ use_priority_weighting\n",
        "        - v3 (2025-02-09): æ•´åˆ enable_gateï¼Œçµ±ä¸€ç‚ºå–®ä¸€å‡½æ•¸\n",
        "    \"\"\"\n",
        "    if llm_config is None:\n",
        "        llm_config = SELECTED_CONFIG  # â† [BUG FIX] åŸæœ¬ v2 èª¤ç”¨æ•´å€‹ LLM_CONFIG dict\n",
        "\n",
        "    # ===== Step 0: Pre-LLM Gate =====\n",
        "    if enable_gate:\n",
        "        if verbose:\n",
        "            print(\"=\" * 60)\n",
        "            print(\"ğŸš¦ Step 0: Pre-LLM Gate æª¢æŸ¥...\")\n",
        "\n",
        "        gate_result = pre_llm_gate(scenario, verbose=verbose)\n",
        "\n",
        "        if gate_result.decision == GateDecision.REFUSE:\n",
        "            if verbose:\n",
        "                print(f\"   ğŸš« æ±ºç­–: REFUSE\")\n",
        "                print(f\"   åŸå› : {gate_result.reason_message}\")\n",
        "                print(\"=\" * 60)\n",
        "            return gate_result.to_refuse_response()\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   âœ… æ±ºç­–: ALLOW\")\n",
        "\n",
        "    # ===== Step 1: æª¢ç´¢ =====\n",
        "    if verbose:\n",
        "        if not enable_gate:\n",
        "            print(\"=\" * 60)\n",
        "        print(\"ğŸ” Step 1: æª¢ç´¢ç›¸é—œæ®µè½...\")\n",
        "\n",
        "    contexts = retrieve_contexts(\n",
        "        query=scenario,\n",
        "        faiss_index=faiss_index,\n",
        "        chunks=chunks,\n",
        "        embedding_model=embedding_model,\n",
        "        bm25_index=bm25_index,\n",
        "        tokenized_corpus=tokenized_corpus,\n",
        "        k=k,\n",
        "        method=retrieval_method,\n",
        "        use_priority_weighting=use_priority_weighting,\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"   æ‰¾åˆ° {len(contexts)} å€‹ç›¸é—œæ®µè½\")\n",
        "        for i, ctx in enumerate(contexts, 1):\n",
        "            print(\n",
        "                f\"   [{i}] {ctx.get('doc_category', 'unknown'):15} \"\n",
        "                f\"| {ctx['source']} p.{ctx['page']} \"\n",
        "                f\"(score: {ctx['score']:.3f})\"\n",
        "            )\n",
        "\n",
        "    # ===== Step 2: çµ„è£ Prompt =====\n",
        "    if verbose:\n",
        "        print(\"\\nğŸ“ Step 2: çµ„è£ Prompt...\")\n",
        "\n",
        "    user_prompt = build_user_prompt(scenario, contexts)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"   Prompt é•·åº¦: {len(user_prompt)} å­—å…ƒ\")\n",
        "        print(f\"\\nğŸ¤– Step 3: å‘¼å« LLM ({llm_config['llm_model_name']})...\")\n",
        "\n",
        "    # ===== Step 3: å‘¼å« LLM =====\n",
        "    result = call_llm(SYSTEM_PROMPT, user_prompt, llm_config)  # â† [BUG FIX] çµ±ä¸€ç”¨ SYSTEM_PROMPT\n",
        "\n",
        "    # é™„åŠ æª¢ç´¢è³‡è¨Šï¼ˆä¾›å¾ŒçºŒè¨ºæ–·ï¼‰\n",
        "    result[\"_retrieved_chunks\"] = [\n",
        "        {\"chunk_id\": ctx[\"chunk_id\"], \"source\": ctx[\"source\"],\n",
        "         \"page\": ctx[\"page\"], \"score\": ctx[\"score\"]}\n",
        "        for ctx in contexts\n",
        "    ]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"   âœ… åˆ†æå®Œæˆ\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GT_IA5Xg4fd"
      },
      "source": [
        "### 3.5 pretty_print_result()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1zDh8ohDh4N"
      },
      "outputs": [],
      "source": [
        "def pretty_print_result(result: dict):\n",
        "    \"\"\"ç¾åŒ–è¼¸å‡ºåˆ†æçµæœ\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ“‹ AML ç´…æ——åˆ†æå ±å‘Š\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(f\"\\nğŸ“Œ æƒ…å¢ƒæ‘˜è¦ï¼š{result.get('scenario_summary', 'N/A')}\")\n",
        "\n",
        "    assessment = result.get(\"assessment\", \"unknown\")\n",
        "    emoji_map = {\n",
        "        \"confirmed\": \"ğŸš¨\", \"possible\": \"âš ï¸\", \"unlikely\": \"âœ…\",\n",
        "        \"refuse\": \"ğŸš«\", \"error\": \"âŒ\",\n",
        "    }\n",
        "    print(f\"\\nğŸ¯ ç¶œåˆè©•ä¼°ï¼š{emoji_map.get(assessment, 'â“')} {assessment.upper()}\")\n",
        "\n",
        "    flags = result.get(\"identified_flags\", [])\n",
        "    if flags:\n",
        "        print(f\"\\nğŸš© è­˜åˆ¥åˆ°çš„ç´…æ—— ({len(flags)} å€‹)ï¼š\")\n",
        "        print(\"-\" * 40)\n",
        "        for flag in flags:\n",
        "            conf_emoji = {\"high\": \"ğŸ”´\", \"medium\": \"ğŸŸ¡\", \"low\": \"ğŸŸ¢\"}.get(\n",
        "                flag.get(\"confidence\"), \"âšª\"\n",
        "            )\n",
        "            print(f\"\\n  {conf_emoji} [{flag.get('flag_id')}] {flag.get('flag_name')}\")\n",
        "            print(f\"     åˆ†æï¼š{flag.get('reasoning', 'N/A')}\")\n",
        "\n",
        "            for ev in flag.get(\"evidence\", []):\n",
        "                quote = ev.get(\"quote\", \"\")[:50]\n",
        "                print(f\"     ğŸ“š {ev.get('source')} p.{ev.get('page')}: ã€Œ{quote}...ã€\")\n",
        "\n",
        "    questions = result.get(\"follow_up_questions\", [])\n",
        "    if questions:\n",
        "        print(f\"\\nâ“ å»ºè­°è¿½å•ï¼š\")\n",
        "        for i, q in enumerate(questions, 1):\n",
        "            print(f\"   {i}. {q}\")\n",
        "\n",
        "    print(f\"\\nğŸ“œ {result.get('disclaimer', 'æœ¬åˆ†æåƒ…ä¾›æ•™è‚²åƒè€ƒï¼Œä¸æ§‹æˆæ³•å¾‹æ„è¦‹ã€‚')}\")\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPEn_0iKD6as"
      },
      "outputs": [],
      "source": [
        "print(\"\\nâœ… PART 3: LLM åˆ†ææ¨¡çµ„å·²è¼‰å…¥\")\n",
        "print(f\"âš™ï¸  ä½¿ç”¨ analyze_scenario() çµ±ä¸€å…¥å£\")\n",
        "print(f\"   æ”¯æ´åƒæ•¸: use_priority_weighting / enable_gate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nTmBmGOg7B2"
      },
      "source": [
        "# ğŸ§ª PART 4: æ¸¬è©¦æ¡ˆä¾‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyBP-T8GhBRW"
      },
      "source": [
        "é€™ 16 é¡Œæ˜¯æ¸¬è©¦ã€Œæ•´é«”ç³»çµ±ã€çš„â€”â€”LLM èƒ½ä¸èƒ½æ­£ç¢ºåˆ†ææƒ…å¢ƒï¼Ÿ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abLfXt-xhCse"
      },
      "source": [
        "### 4.1 æ¸¬è©¦æ¡ˆä¾‹å®šç¾©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMT67w1zhICV"
      },
      "outputs": [],
      "source": [
        "END_TO_END_TEST_CASES = {\n",
        "    # ========== RF-01: é–€æª»æ‹†åˆ† (Structuring) ==========\n",
        "    \"1A\": {\n",
        "        \"name\": \"é–€æª»æ‹†åˆ† - confirmed\",\n",
        "        \"scenario\": \"\"\"\n",
        "æŸå®¢æˆ¶åœ¨åŒä¸€å¤©å…§åˆ† 6 æ¬¡è‡¨æ«ƒå­˜å…¥ç¾é‡‘ï¼Œæ¯ç­†éƒ½ç•¥ä½æ–¼å…§éƒ¨ç”³å ±é–€æª»ã€‚\n",
        "å­˜å®Œå¾Œ 30 åˆ†é˜å…§ï¼Œå°‡å¤§éƒ¨åˆ†é‡‘é¡åˆ†å…©ç­†è½‰å‡ºåˆ°ä¸åŒå¸³æˆ¶ã€‚\n",
        "å®¢æˆ¶è¢«å•åˆ°è³‡é‡‘ä¾†æºæ™‚ï¼Œåªå›ã€Œæœ‹å‹å€Ÿæˆ‘çš„ã€ï¼Œç„¡å…¶ä»–è­‰æ˜ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"confirmed\",\n",
        "        \"expected_flags\": [\"RF-01\", \"RF-02\"],\n",
        "        \"required_sources\": [\"TW_GOV\", \"FATF_VA\"]\n",
        "    },\n",
        "    \"1B\": {\n",
        "        \"name\": \"é–€æª»æ‹†åˆ† - possible\",\n",
        "        \"scenario\": \"\"\"\n",
        "æŸå®¢æˆ¶æœ€è¿‘é–‹å§‹é »ç¹å­˜ç¾ï¼Œä½†æ¯æ¬¡é‡‘é¡éƒ½ä¸å¤§ã€‚\n",
        "å®¢æˆ¶èªªè‡ªå·±åšå°ç”Ÿæ„ï¼Œç¾é‡‘å¤šå¾ˆæ­£å¸¸ã€‚\n",
        "ä½ åªçŸ¥é“ã€Œæœ€è¿‘å¸¸å­˜ç¾ã€ï¼Œä¸çŸ¥é“é »ç‡èˆ‡é‡‘é¡åˆ†å¸ƒã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"possible\",\n",
        "        \"expected_flags\": [\"RF-01\"],\n",
        "        \"required_sources\": [\"TW_GOV\"]\n",
        "    },\n",
        "\n",
        "    # ========== RF-02: å¿«é€Ÿæµè½‰ (Rapid Movement) ==========\n",
        "    \"2A\": {\n",
        "        \"name\": \"å¿«é€Ÿæµè½‰ - confirmed\",\n",
        "        \"scenario\": \"\"\"\n",
        "æŸå¸³æˆ¶å‡Œæ™¨æ”¶åˆ°ä¸€ç­†å¤§é¡å…¥å¸³ï¼Œ1 å°æ™‚å…§åˆ†æ•£è½‰å‡ºåˆ° 8 å€‹ä¸åŒå°æ‰‹æ–¹ã€‚\n",
        "éš”å¤©åˆæœ‰å¦ä¸€ç­†å¤§é¡å…¥å¸³ï¼Œé‡è¤‡ç›¸åŒæ¨¡å¼ã€‚\n",
        "å¸³æˆ¶æŒæœ‰äººèªªã€Œåªæ˜¯å¹«æœ‹å‹å‘¨è½‰ã€ï¼Œä¸æ¸…æ¥šè³‡é‡‘ç”¨é€”ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"confirmed\",\n",
        "        \"expected_flags\": [\"RF-02\"],\n",
        "        \"required_sources\": [\"FATF_VA\", \"FATF_TBML\"]\n",
        "    },\n",
        "    \"2B\": {\n",
        "        \"name\": \"å¿«é€Ÿæµè½‰ - possible\",\n",
        "        \"scenario\": \"\"\"\n",
        "æŸå¸³æˆ¶æ”¶åˆ°ä¸€ç­†å…¥å¸³å¾Œå¾ˆå¿«è½‰å‡ºï¼Œä½†ä½ ä¸ç¢ºå®šè½‰å‡ºå°è±¡æœ‰å¹¾å€‹ã€‚\n",
        "å®¢æˆ¶èªªè½‰å‡ºæ˜¯ã€Œä»˜è²¨æ¬¾ã€ï¼Œä½†æ²’æä¾›ç™¼ç¥¨æˆ–åˆç´„ã€‚\n",
        "ä½ ä¹Ÿä¸ç¢ºå®šé€™æ˜¯å¦ç‚ºè©²å®¢æˆ¶å¸¸æ…‹è¡Œç‚ºã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"possible\",\n",
        "        \"expected_flags\": [\"RF-02\"],\n",
        "        \"required_sources\": [\"TW_GOV\"]\n",
        "    },\n",
        "\n",
        "    # ========== RF-03: ç¾é‡‘å¯†é›†ä¸”èˆ‡å®¢ç¾¤ä¸ç¬¦ ==========\n",
        "    \"3A\": {\n",
        "        \"name\": \"ç¾é‡‘ç•°å¸¸ - confirmed\",\n",
        "        \"scenario\": \"\"\"\n",
        "ä¸€åé•·æœŸåªç”¨è½‰å¸³çš„å°é¡ä¸Šç­æ—ï¼Œçªç„¶é–‹å§‹æ¯é€±å¤§é‡å­˜ç¾ã€‚\n",
        "å­˜ç¾å¾Œå¸¸åœ¨ç•¶æ—¥æé ˜æˆ–ç«‹åˆ»è½‰å‡ºï¼Œä¸”æ‹’çµ•èªªæ˜ç¾é‡‘ä¾†æºã€‚\n",
        "å…¶å¸³æˆ¶æ­·å²èˆ‡ç¾é‡‘æµé‡å‹æ…‹æ˜é¡¯ä¸ä¸€è‡´ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"confirmed\",\n",
        "        \"expected_flags\": [\"RF-03\"],\n",
        "        \"required_sources\": [\"TW_GOV\"]\n",
        "    },\n",
        "    \"3B\": {\n",
        "        \"name\": \"ç¾é‡‘ç•°å¸¸ - possible\",\n",
        "        \"scenario\": \"\"\"\n",
        "æŸå®¢æˆ¶å¸¸ä»¥ç¾é‡‘äº¤æ˜“ï¼Œä½†ä»–èªªè‡ªå·±æ˜¯åšé¤é£²å°åº—ã€‚\n",
        "ä½ æ²’æœ‰ä»–çš„ç‡Ÿæ”¶è¦æ¨¡ï¼Œä¹Ÿæ²’æœ‰ç¾é‡‘å­˜å…¥èˆ‡ç‡Ÿæ¥­å‹æ…‹çš„å°ç…§ã€‚\n",
        "ä½ åªçŸ¥é“ã€Œç¾é‡‘å¾ˆå¤šã€ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"possible\",\n",
        "        \"expected_flags\": [\"RF-03\"],\n",
        "        \"required_sources\": [\"TW_GOV\"]\n",
        "    },\n",
        "\n",
        "    # ========== RF-04: ç¬¬ä¸‰äººä»£è¾¦ / äººé ­å¸³æˆ¶ ==========\n",
        "    \"4A\": {\n",
        "        \"name\": \"ç¬¬ä¸‰äººä»£è¾¦ - confirmed\",\n",
        "        \"scenario\": \"\"\"\n",
        "æŸå¸³æˆ¶æŒæœ‰äººæ˜¯å­¸ç”Ÿï¼Œä½†äº¤æ˜“å¤šç”±ã€Œå”å”ã€ä»£ç‚ºæ“ä½œã€‚\n",
        "äº¤æ˜“å°è±¡å¤šç‚ºé™Œç”Ÿå…¬å¸å¸³æˆ¶ï¼Œä¸”è³‡é‡‘å¸¸åœ¨å…¥å¸³å¾Œè¿…é€Ÿè½‰å‡ºã€‚\n",
        "ç•¶è¢«å•åˆ°å—ç›Šäººèˆ‡è³‡é‡‘ç”¨é€”æ™‚ï¼ŒæŒæœ‰äººç„¡æ³•èªªæ˜ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"confirmed\",\n",
        "        \"expected_flags\": [\"RF-04\", \"RF-02\"],\n",
        "        \"required_sources\": [\"TW_GOV\", \"FATF_VA\"]\n",
        "    },\n",
        "    \"4B\": {\n",
        "        \"name\": \"ç¬¬ä¸‰äººä»£è¾¦ - possible\",\n",
        "        \"scenario\": \"\"\"\n",
        "å®¢æˆ¶èªªæ˜¯ã€Œå¹«å®¶äººæ”¶æ¬¾ã€ï¼Œä½†ä½ ä¸çŸ¥é“å®¶äººèº«åˆ†ã€‚\n",
        "å¸³æˆ¶æœ‰è½‰å…¥è½‰å‡ºï¼Œä½†é‡‘é¡å¤§å°èˆ‡é »ç‡æœªæä¾›ã€‚\n",
        "å®¢æˆ¶ä¸é¡˜æ„ç•™ä¸‹ä»»ä½•è¯çµ¡è³‡è¨Šã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"possible\",\n",
        "        \"expected_flags\": [\"RF-04\"],\n",
        "        \"required_sources\": [\"TW_GOV\"]\n",
        "    },\n",
        "\n",
        "    # ========== RF-05: è·¨å¢ƒ / é«˜é¢¨éšªåœ°å€ ==========\n",
        "    \"5A\": {\n",
        "        \"name\": \"è·¨å¢ƒé«˜é¢¨éšª - confirmed\",\n",
        "        \"scenario\": \"\"\"\n",
        "æŸå®¢æˆ¶åœ¨çŸ­æ™‚é–“å…§å°‡è³‡é‡‘åˆ†æ‰¹åŒ¯å¾€å¤šå€‹å¢ƒå¤–å¸³æˆ¶ã€‚\n",
        "å…¶ä¸­åŒ…å«é«˜é¢¨éšªæˆ–éåˆä½œåœ°å€ï¼Œä¸”å®¢æˆ¶ç„¡æ³•èªªæ˜è²¿æ˜“æˆ–æŠ•è³‡ç†ç”±ã€‚\n",
        "åŒ¯å‡ºå¾Œåˆå¾ˆå¿«æœ‰è³‡é‡‘å›æµåˆ°å…¶ä»–å¸³æˆ¶ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"confirmed\",\n",
        "        \"expected_flags\": [\"RF-05\"],\n",
        "        \"required_sources\": [\"FATF_VA\", \"TW_GOV\"]\n",
        "    },\n",
        "    \"5B\": {\n",
        "        \"name\": \"è·¨å¢ƒé«˜é¢¨éšª - possible\",\n",
        "        \"scenario\": \"\"\"\n",
        "å®¢æˆ¶èªªè¦åŒ¯æ¬¾åˆ°æµ·å¤–è¦ªå‹ï¼Œä½†æ²’æœ‰æä¾›è¦ªå‹é—œä¿‚è­‰æ˜ã€‚\n",
        "ä½ ä¸çŸ¥é“åŒ¯æ¬¾é »ç‡ï¼Œä¹Ÿä¸çŸ¥é“æ”¶æ¬¾åœ°å€é¢¨éšªå±¬æ€§ã€‚\n",
        "å®¢æˆ¶è¡¨ç¤ºã€Œæ€¥ç”¨éŒ¢ã€è¦æ±‚ç«‹å³è™•ç†ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"possible\",\n",
        "        \"expected_flags\": [\"RF-05\"],\n",
        "        \"required_sources\": [\"TW_GOV\"]\n",
        "    },\n",
        "\n",
        "    # ========== RF-06: èˆ‡èº«åˆ† / å•†æ¥­æ¨¡å¼ä¸ç¬¦ ==========\n",
        "    \"6A\": {\n",
        "        \"name\": \"èˆ‡èº«åˆ†ä¸ç¬¦ - confirmed\",\n",
        "        \"scenario\": \"\"\"\n",
        "æŸå°å‹æ–‡å‰µå·¥ä½œå®¤å¸³æˆ¶ï¼Œå»æŒçºŒæ”¶å–å¤§é‡èˆ‡å…¶æ¥­å‹™ç„¡é—œçš„æ¬¾é …ã€‚\n",
        "æ”¶æ¬¾å¾Œå¤§å¤šç«‹å³è½‰å‡ºè‡³ä¸åŒå€‹äººå¸³æˆ¶ï¼Œä¸”ç„¡åˆç´„æˆ–ç™¼ç¥¨ã€‚\n",
        "è² è²¬äººç„¡æ³•èªªæ˜äº¤æ˜“å°æ‰‹èˆ‡è³‡é‡‘ç”¨é€”ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"confirmed\",\n",
        "        \"expected_flags\": [\"RF-06\", \"RF-02\"],\n",
        "        \"required_sources\": [\"FATF_TBML\", \"TW_GOV\"]\n",
        "    },\n",
        "    \"6B\": {\n",
        "        \"name\": \"èˆ‡èº«åˆ†ä¸ç¬¦ - possible\",\n",
        "        \"scenario\": \"\"\"\n",
        "å®¢æˆ¶èªªè‡ªå·±æ˜¯è‡ªç”±æ¥æ¡ˆè€…ï¼Œæ‰€ä»¥æ”¶æ¬¾ä¾†æºå¾ˆå¤šã€‚\n",
        "ä½ ä¸çŸ¥é“é€™äº›ä¾†æºæ˜¯å¹³å°æ’¥æ¬¾ã€é‚„æ˜¯é™Œç”Ÿå€‹äººåŒ¯æ¬¾ã€‚\n",
        "ä½ ä¹Ÿæ²’æœ‰å®¢æˆ¶éå¾€äº¤æ˜“å‹æ…‹å¯å°ç…§ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"possible\",\n",
        "        \"expected_flags\": [\"RF-06\"],\n",
        "        \"required_sources\": [\"TW_GOV\"]\n",
        "    },\n",
        "\n",
        "    # ========== RF-07: è™›æ“¬è³‡ç”¢åŒ¿åæ€§ ==========\n",
        "    \"7A\": {\n",
        "        \"name\": \"è™›æ“¬è³‡ç”¢åŒ¿å - confirmed\",\n",
        "        \"scenario\": \"\"\"\n",
        "å®¢æˆ¶å°‡å¤šç­†è³‡é‡‘è½‰å…¥åŠ å¯†è²¨å¹£äº¤æ˜“æ‰€å¾Œï¼Œç«‹åˆ»æåˆ°éè¨—ç®¡éŒ¢åŒ…ã€‚\n",
        "éš”å¤©åˆå¾ä¸åŒåœ°å€æŠŠè³‡é‡‘æ‰“å›ä¾†ï¼Œä¸¦èªªä¸æ¸…æ¥šäº¤æ˜“å°è±¡æ˜¯èª°ã€‚\n",
        "å®¢æˆ¶æåˆ°ä½¿ç”¨ OTC ç§ä¸‹äº¤æ˜“ä¸”ä¸ç•™ç´€éŒ„ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"confirmed\",\n",
        "        \"expected_flags\": [\"RF-07\"],\n",
        "        \"required_sources\": [\"FATF_VA\"]\n",
        "    },\n",
        "    \"7B\": {\n",
        "        \"name\": \"è™›æ“¬è³‡ç”¢åŒ¿å - possible\",\n",
        "        \"scenario\": \"\"\"\n",
        "å®¢æˆ¶èªªè‡ªå·±æœ‰åœ¨è²·æ¯”ç‰¹å¹£ï¼Œä½†åªæ˜¯é•·æœŸæŠ•è³‡ã€‚\n",
        "ä½ åªçŸ¥é“ä»–æœ‰å‡ºå…¥é‡‘äº¤æ˜“æ‰€ï¼Œå»ä¸çŸ¥é“é »ç‡èˆ‡é‡‘é¡ã€‚\n",
        "ä½ ä¹Ÿä¸çŸ¥é“ä»–æ˜¯å¦æåˆ°å¤–éƒ¨éŒ¢åŒ…æˆ–ç§ä¸‹äº¤æ˜“ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"possible\",\n",
        "        \"expected_flags\": [\"RF-07\"],\n",
        "        \"required_sources\": [\"FATF_VA\", \"TW_GOV\"]\n",
        "    },\n",
        "\n",
        "    # ========== RF-08: å…¬å¸ / å—ç›Šæ‰€æœ‰äººä¸é€æ˜ ==========\n",
        "    \"8A\": {\n",
        "        \"name\": \"å…¬å¸ä¸é€æ˜ - confirmed\",\n",
        "        \"scenario\": \"\"\"\n",
        "æŸæ–°æˆç«‹å…¬å¸è¦æ±‚é–‹æˆ¶ä¸¦å¿«é€Ÿé€²è¡Œå¤§é¡æ”¶ä»˜æ¬¾ï¼Œä½†ç„¡æ˜ç¢ºç‡Ÿæ¥­å…§å®¹ã€‚\n",
        "è² è²¬äººç„¡æ³•èªªæ˜å¯¦éš›å—ç›Šäººï¼Œè‚¡æ¬Šçµæ§‹å¤šå±¤ä¸”é›£ä»¥é‡æ¸…ã€‚\n",
        "äº¤æ˜“å°æ‰‹éå¸ƒå¤šåœ°ï¼Œä¸”ç”¨é€”æ•˜è¿°åè¦†è®Šæ›´ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"confirmed\",\n",
        "        \"expected_flags\": [\"RF-08\"],\n",
        "        \"required_sources\": [\"FATF_TBML\", \"TW_GOV\"]\n",
        "    },\n",
        "    \"8B\": {\n",
        "        \"name\": \"TBML è¶…å‡ºç¯„åœ - refuse\",\n",
        "        \"scenario\": \"\"\"\n",
        "å®¢æˆ¶å¾äº‹ã€Œåœ‹éš›è²¿æ˜“ã€ï¼Œäº¤æ˜“æ¶‰åŠå¤šç¨®è²¨ç‰©èˆ‡åœ‹å®¶ã€‚\n",
        "ä½ æ‡·ç–‘å¯èƒ½æœ‰è²¿æ˜“å‹æ´—éŒ¢ï¼ˆTBMLï¼‰ï¼Œä½†ä½ æ‰‹ä¸Šçš„æ•™ææ²’æœ‰ä»»ä½• TBML ç« ç¯€ã€‚\n",
        "ä½ ä¹Ÿæ²’æœ‰å ±é—œå–®ã€ç™¼ç¥¨ã€è²¨ç‰©æµå‘ç­‰è³‡æ–™ã€‚\n",
        "\"\"\",\n",
        "        \"expected_assessment\": \"refuse\",\n",
        "        \"expected_flags\": [],\n",
        "        \"required_sources\": []\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38SVeGiChK3F"
      },
      "source": [
        "### 4.2 å–®ä¸€æƒ…å¢ƒæ¸¬è©¦\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gLgXxYBHZ_l"
      },
      "outputs": [],
      "source": [
        "# æ¸¬è©¦ confirmed æ¡ˆä¾‹\n",
        "print(\"âš™ï¸ æœ¬æ¬¡å¯¦é©—è¨­å®š:\")\n",
        "print(f\"   LLM: {SELECTED_CONFIG['llm_model_name']}\")\n",
        "print(f\"   Priority Weighting: {DEFAULT_USE_PRIORITY_WEIGHTING}\")\n",
        "print(f\"   Gate: {DEFAULT_ENABLE_GATE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1VmfbXvtTXX"
      },
      "outputs": [],
      "source": [
        "result = analyze_scenario(\n",
        "    scenario=END_TO_END_TEST_CASES[\"2B\"][\"scenario\"],\n",
        "    faiss_index=faiss_index,\n",
        "    chunks=chunks,\n",
        "    embedding_model=embedding_model,\n",
        "    bm25_index=bm25_index,\n",
        "    tokenized_corpus=tokenized_corpus,\n",
        "    llm_config=SELECTED_CONFIG,\n",
        "    use_priority_weighting=True,\n",
        "    enable_gate=False,       # â† å•Ÿç”¨ Gate\n",
        ")\n",
        "pretty_print_result(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E2ams6RtTGn"
      },
      "outputs": [],
      "source": [
        "result = analyze_scenario(\n",
        "    scenario=END_TO_END_TEST_CASES[\"6A\"][\"scenario\"],\n",
        "    faiss_index=faiss_index,\n",
        "    chunks=chunks,\n",
        "    embedding_model=embedding_model,\n",
        "    bm25_index=bm25_index,\n",
        "    tokenized_corpus=tokenized_corpus,\n",
        "    llm_config=SELECTED_CONFIG,\n",
        "    use_priority_weighting=True,\n",
        "    enable_gate=False,       # â† å•Ÿç”¨ Gate\n",
        ")\n",
        "pretty_print_result(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E8YJbzuGWCz"
      },
      "outputs": [],
      "source": [
        "# æ¸¬è©¦ confirmed æ¡ˆä¾‹\n",
        "result = analyze_scenario(\n",
        "    scenario=END_TO_END_TEST_CASES[\"1A\"][\"scenario\"],\n",
        "    faiss_index=faiss_index,\n",
        "    chunks=chunks,\n",
        "    embedding_model=embedding_model,\n",
        "    bm25_index=bm25_index,\n",
        "    tokenized_corpus=tokenized_corpus,\n",
        "    llm_config=SELECTED_CONFIG,\n",
        "    use_priority_weighting=True,\n",
        "    enable_gate=False,       # â† å•Ÿç”¨ Gate\n",
        ")\n",
        "pretty_print_result(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcI0srnsGbSE"
      },
      "outputs": [],
      "source": [
        "# æ¸¬è©¦ refuse æ¡ˆä¾‹ï¼ˆå« Gateï¼‰\n",
        "result = analyze_scenario(\n",
        "    scenario=END_TO_END_TEST_CASES[\"8B\"][\"scenario\"],\n",
        "    faiss_index=faiss_index,\n",
        "    chunks=chunks,\n",
        "    embedding_model=embedding_model,\n",
        "    bm25_index=bm25_index,\n",
        "    tokenized_corpus=tokenized_corpus,\n",
        "    llm_config=SELECTED_CONFIG,\n",
        "    use_priority_weighting=True,\n",
        "    enable_gate=True,       # â† å•Ÿç”¨ Gate\n",
        ")\n",
        "pretty_print_result(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcaC4IgWhO-s"
      },
      "source": [
        "### 4.3 æ‰¹æ¬¡æ¸¬è©¦ï¼ˆå¯é¸ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cIez388G-Hz"
      },
      "outputs": [],
      "source": [
        "def run_all_tests(\n",
        "    test_cases: dict,\n",
        "    faiss_index,\n",
        "    chunks: list,\n",
        "    embedding_model,\n",
        "    bm25_index,\n",
        "    tokenized_corpus: list,\n",
        "    llm_config: dict,\n",
        "    use_priority_weighting: bool = DEFAULT_USE_PRIORITY_WEIGHTING,\n",
        "    enable_gate: bool = DEFAULT_ENABLE_GATE,\n",
        "    enable_logging: bool = True,\n",
        "    version: str = \"v2.0\",\n",
        "    experiment_type: str = \"baseline\",\n",
        "):\n",
        "    \"\"\"\n",
        "    æ‰¹æ¬¡åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦æ¡ˆä¾‹\n",
        "\n",
        "    Args:\n",
        "        test_cases: END_TO_END_TEST_CASES æ ¼å¼çš„ dict\n",
        "        enable_gate: æ˜¯å¦å•Ÿç”¨ Pre-LLM Gate\n",
        "        enable_logging: æ˜¯å¦å•Ÿç”¨å¯¦é©—è¨˜éŒ„\n",
        "        version: å¯¦é©—ç‰ˆæœ¬è™Ÿ\n",
        "        experiment_type: å¯¦é©—é¡å‹ï¼ˆbaseline / fix / tuning / ab_test / debugï¼‰\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: æ¯å€‹ dict åŒ…å« id, passed, expected, actual, gate_decision\n",
        "\n",
        "    History:\n",
        "        - v1: åŸºç¤æ‰¹æ¬¡æ¸¬è©¦ï¼ˆrun_all_testsï¼‰\n",
        "        - v2: æ•´åˆ Gate æ”¯æ´ï¼ˆåˆä½µ run_all_tests_with_gateï¼‰\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    passed_count = 0\n",
        "    total_count = len(test_cases)\n",
        "\n",
        "    # Logger åˆå§‹åŒ–\n",
        "    logger = None\n",
        "    if enable_logging:\n",
        "        config = {\n",
        "            **llm_config,\n",
        "            \"use_priority_weighting\": use_priority_weighting,\n",
        "            \"enable_gate\": enable_gate,\n",
        "            \"k\": DEFAULT_TOP_K,\n",
        "            \"retrieval_method\": \"hybrid\",\n",
        "            \"rrf_k\": DEFAULT_RRF_K,\n",
        "            \"index_version\": index_metadata.get(\"version\", \"unknown\"),\n",
        "        }\n",
        "\n",
        "        logger = ExperimentLogger(\n",
        "            base_dir=EXPERIMENT_ROOT_DIR,\n",
        "            version=version,\n",
        "            experiment_type=experiment_type,\n",
        "            # config=config,\n",
        "            # auto_describe=True,\n",
        "        )\n",
        "        logger.log_config(config)\n",
        "\n",
        "    for test_id, test in test_cases.items():\n",
        "        print(f\"\\nğŸ§ª æ¸¬è©¦ {test_id}: {test['name']}\")\n",
        "\n",
        "        try:\n",
        "            result = analyze_scenario(\n",
        "                scenario=test[\"scenario\"],\n",
        "                faiss_index=faiss_index,\n",
        "                chunks=chunks,\n",
        "                embedding_model=embedding_model,\n",
        "                bm25_index=bm25_index,\n",
        "                tokenized_corpus=tokenized_corpus,\n",
        "                llm_config=llm_config,\n",
        "                use_priority_weighting=use_priority_weighting,\n",
        "                enable_gate=enable_gate,\n",
        "                verbose=False,\n",
        "            )\n",
        "\n",
        "            actual = result.get(\"assessment\", \"unknown\")\n",
        "            expected = test[\"expected_assessment\"]\n",
        "            passed = actual == expected\n",
        "\n",
        "            if passed:\n",
        "                passed_count += 1\n",
        "\n",
        "            gate_meta = result.get(\"_gate_metadata\", {})\n",
        "            gate_decision = gate_meta.get(\"decision\", \"LLM\")\n",
        "\n",
        "            status = \"âœ…\" if passed else \"âŒ\"\n",
        "            print(f\"   é æœŸ: {expected} | å¯¦éš›: {actual} | ä¾†æº: {gate_decision} | {status}\")\n",
        "\n",
        "            results.append({\n",
        "                \"id\": test_id,\n",
        "                \"passed\": passed,\n",
        "                \"expected\": expected,\n",
        "                \"actual\": actual,\n",
        "                \"gate_decision\": gate_decision,\n",
        "            })\n",
        "\n",
        "            if logger:\n",
        "                logger.log_case(\n",
        "                    case_id=test_id,\n",
        "                    scenario=test[\"scenario\"],\n",
        "                    expected=expected,\n",
        "                    actual=actual,\n",
        "                    passed=passed,\n",
        "                    retrieved_chunks=result.get(\"_retrieved_chunks\", []),\n",
        "                    llm_response={\n",
        "                        \"assessment\": actual,\n",
        "                        \"flags\": result.get(\"identified_flags\", []),\n",
        "                    },\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ éŒ¯èª¤: {e}\")\n",
        "            results.append({\"id\": test_id, \"passed\": False, \"error\": str(e)})\n",
        "\n",
        "            if logger:\n",
        "                logger.log_case(\n",
        "                    case_id=test_id,\n",
        "                    scenario=test[\"scenario\"],\n",
        "                    expected=test[\"expected_assessment\"],\n",
        "                    actual=\"error\",\n",
        "                    passed=False,\n",
        "                    error=str(e),\n",
        "                )\n",
        "\n",
        "    # çµ±è¨ˆ\n",
        "    accuracy = passed_count / total_count if total_count > 0 else 0\n",
        "    gate_refused = sum(1 for r in results if r.get(\"gate_decision\") == \"REFUSE\")\n",
        "\n",
        "    print(f\"\\nğŸ“Š ç¸½çµ: {passed_count}/{total_count} é€šé ({accuracy:.1%})\")\n",
        "    if enable_gate:\n",
        "        print(f\"ğŸš¦ Gate æ””æˆª: {gate_refused} å€‹æ¡ˆä¾‹\")\n",
        "\n",
        "    if logger:\n",
        "        metrics = {\n",
        "            \"total_cases\": total_count,\n",
        "            \"passed\": passed_count,\n",
        "            \"failed\": total_count - passed_count,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"gate_refused\": gate_refused,\n",
        "        }\n",
        "        logger.log_metrics(metrics)\n",
        "        logger.log_batch_results(results)\n",
        "        logger.finalize(summary=f\"æº–ç¢ºç‡: {accuracy:.1%} ({passed_count}/{total_count})\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hh2YSNqHB0r"
      },
      "outputs": [],
      "source": [
        "# åŸ·è¡Œæ‰¹æ¬¡æ¸¬è©¦\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸš€ PART 4: æ‰¹æ¬¡æ¸¬è©¦æ‰€æœ‰æƒ…å¢ƒ...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_test_results = run_all_tests(\n",
        "    test_cases=END_TO_END_TEST_CASES,\n",
        "    faiss_index=faiss_index,\n",
        "    chunks=chunks,\n",
        "    embedding_model=embedding_model,\n",
        "    bm25_index=bm25_index,\n",
        "    tokenized_corpus=tokenized_corpus,\n",
        "    llm_config=SELECTED_CONFIG,\n",
        "    use_priority_weighting=True,\n",
        "    enable_gate=True,            # â† å•Ÿç”¨ Gate\n",
        "    enable_logging=ENABLE_LOGGING\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… æ‰€æœ‰æ‰¹æ¬¡æ¸¬è©¦å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vDHWZeKpIGn"
      },
      "source": [
        "# PART 5: è¨ºæ–·å·¥å…·ï¼ˆå¯é¸åŸ·è¡Œï¼‰\n",
        "é€™éƒ¨åˆ†ç”¨æ–¼é©—è­‰ã€Œæ¬Šé‡èª¿æ•´ã€æ˜¯å¦çœŸçš„æ”¹å–„äº†ç³»çµ±è¡¨ç¾ã€‚\n",
        "åœ¨ç¢ºèªæœ‰æ•ˆå¾Œï¼Œå¯ä»¥ä¸åŸ·è¡Œé€™éƒ¨åˆ†ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN9jO6qjqfat"
      },
      "source": [
        "ä»¥ä¸‹æƒ…æ³ç™¼ç”Ÿæ™‚ï¼Œå¿…é ˆé€²è¡Œè¨ºæ–·ï¼š\n",
        "\n",
        "* æ–°å¢è³‡æ–™æºï¼šå¦‚æœä½ å¼•é€²äº†ç¬¬ä¸‰ç¨®è³‡æ–™ï¼ˆä¾‹å¦‚ï¼šéŠ€è¡Œå…§éƒ¨è¦ç« ï¼‰ï¼Œä½ éœ€è¦é‡è·‘è¨ºæ–·ï¼Œç¢ºèªæ–°è³‡æ–™ä¸æœƒæŠŠ core æ³•è¦æ“ ä¸‹å»ã€‚\n",
        "\n",
        "* æ›´æ› Embedding æ¨¡å‹ï¼šå¦‚æœä½ æ”¹ç”¨äº†ä¸åŒå» ç‰Œçš„å‘é‡æ¨¡å‹ï¼Œç©ºé–“åˆ†ä½ˆæœƒè®Šï¼Œæ¬Šé‡å¯èƒ½éœ€è¦å¾®èª¿ã€‚\n",
        "\n",
        "* ç™¼ç¾æ–°çš„ã€Œå¹»è¦ºã€ï¼šå¦‚æœä¹‹å¾Œæœ‰é¡Œç›®è©²æ‹’ç­”å»æ²’æ‹’ç­”ï¼Œå°±å¾—å›ä¾†é€™çœ‹æ˜¯ä¸æ˜¯ knowledge_bridge åˆçˆ¬åˆ°ç¬¬ä¸€åäº†ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exC9oRq4pVMP"
      },
      "source": [
        "### 5.1 å–®ä¸€æ¡ˆä¾‹æª¢ç´¢è¨ºæ–·"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFTxJldkpNoH"
      },
      "outputs": [],
      "source": [
        "def diagnose_single_case(test_id: str, show_chunks: bool = False):\n",
        "    \"\"\"\n",
        "    è¨ºæ–·å–®å€‹æ¸¬è©¦æ¡ˆä¾‹çš„æª¢ç´¢çµæœ\n",
        "    å°æ¯”æœ‰æ¬Šé‡ vs ç„¡æ¬Šé‡çš„æ’åºå·®ç•°\n",
        "    \"\"\"\n",
        "    test = END_TO_END_TEST_CASES[test_id]\n",
        "\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"ğŸ” è¨ºæ–· {test_id}: {test['name']}\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    print(f\"\\nğŸ“‹ é æœŸçµæœï¼š\")\n",
        "    print(f\"   Assessment: {test['expected_assessment']}\")\n",
        "    print(f\"   Flags: {test['expected_flags']}\")\n",
        "    print(f\"   Required Sources: {test.get('required_sources', [])}\")\n",
        "\n",
        "    # ç„¡æ¬Šé‡\n",
        "    results_unweighted = hybrid_search(\n",
        "        query=test[\"scenario\"],\n",
        "        faiss_index=faiss_index,\n",
        "        chunks=chunks,\n",
        "        embedding_model=embedding_model,\n",
        "        bm25_index=bm25_index,\n",
        "        tokenized_corpus=tokenized_corpus,\n",
        "        k=5,\n",
        "        use_priority_weighting=False,\n",
        "    )\n",
        "\n",
        "    print(f\"\\n  ã€ç„¡æ¬Šé‡èª¿æ•´ã€‘\")\n",
        "    for i, r in enumerate(results_unweighted, 1):\n",
        "        chunk = r[\"chunk\"]\n",
        "        cat = chunk.get(\"doc_category\", \"unknown\")\n",
        "        print(f\"    [{i}] {cat:18} | Score: {r['score']:.4f} | {chunk.get('source')} p.{chunk.get('page')}\")\n",
        "\n",
        "    # æœ‰æ¬Šé‡\n",
        "    results_weighted = hybrid_search(\n",
        "        query=test[\"scenario\"],\n",
        "        faiss_index=faiss_index,\n",
        "        chunks=chunks,\n",
        "        embedding_model=embedding_model,\n",
        "        bm25_index=bm25_index,\n",
        "        tokenized_corpus=tokenized_corpus,\n",
        "        k=5,\n",
        "        use_priority_weighting=True,\n",
        "    )\n",
        "\n",
        "    print(f\"\\n  ã€æœ‰æ¬Šé‡èª¿æ•´ã€‘\")\n",
        "    for i, r in enumerate(results_weighted, 1):\n",
        "        chunk = r[\"chunk\"]\n",
        "        cat = chunk.get(\"doc_category\", \"unknown\")\n",
        "        priority = r.get(\"priority_weight\", 1.0)\n",
        "        raw = r.get(\"raw_rrf_score\", 0)\n",
        "        print(f\"    [{i}] {cat:18} | Score: {r['score']:.4f} (raw: {raw:.4f} Ã— {priority:.1f}) | {chunk.get('source')} p.{chunk.get('page')}\")\n",
        "\n",
        "    # åˆ†æè®ŠåŒ–\n",
        "    print(f\"\\n  ğŸ“Š æ’åºè®ŠåŒ–ï¼š\")\n",
        "    unweighted_cats = [r[\"chunk\"].get(\"doc_category\") for r in results_unweighted]\n",
        "    weighted_cats = [r[\"chunk\"].get(\"doc_category\") for r in results_weighted]\n",
        "\n",
        "    if unweighted_cats != weighted_cats:\n",
        "        print(f\"    âš ï¸ æ¬Šé‡èª¿æ•´æ”¹è®Šäº†æ’åº\")\n",
        "        for i, (u_cat, w_cat) in enumerate(zip(unweighted_cats, weighted_cats), 1):\n",
        "            if u_cat != w_cat:\n",
        "                print(f\"       æ’å{i}: {u_cat} â†’ {w_cat}\")\n",
        "    else:\n",
        "        print(f\"    âœ… æ¬Šé‡èª¿æ•´æ²’æœ‰æ”¹è®Šæ’åº\")\n",
        "\n",
        "    if show_chunks:\n",
        "        print(f\"\\n  ğŸ“ Chunk å…§å®¹é è¦½ï¼ˆåŠ æ¬Šå¾Œï¼‰ï¼š\")\n",
        "        for i, r in enumerate(results_weighted, 1):\n",
        "            print(f\"\\n    ã€Chunk {i}ã€‘\")\n",
        "            print(f\"    {r['chunk']['text'][:200]}...\")\n",
        "\n",
        "    return results_weighted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPFpX1w1pcJx"
      },
      "outputs": [],
      "source": [
        "# # è¨ºæ–·å¹¾å€‹é—œéµæ¡ˆä¾‹\n",
        "# print(\"\\n\" + \"=\" * 70)\n",
        "# print(\"ğŸ”¬ PART 4-Extra: è¨ºæ–·æ¸¬è©¦\")\n",
        "# print(\"=\" * 70)\n",
        "\n",
        "# diagnose_single_case(\"8A\", show_chunks=False)\n",
        "# diagnose_single_case(\"8B\", show_chunks=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPo0puP9pm95"
      },
      "source": [
        "### 5.2 A/B å°æ¯”æ¸¬è©¦"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjZ8py63pnux"
      },
      "outputs": [],
      "source": [
        "def ab_test_comparison(test_cases: dict):\n",
        "    \"\"\"å°æ¯”æœ‰æ¬Šé‡ vs ç„¡æ¬Šé‡çš„æ¸¬è©¦çµæœ\"\"\"\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"âš–ï¸ A/B å°æ¯”æ¸¬è©¦ï¼šæ¬Šé‡èª¿æ•´çš„å½±éŸ¿\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    # ç„¡æ¬Šé‡ç‰ˆæœ¬\n",
        "    print(f\"\\nã€ç‰ˆæœ¬ Aï¼šç„¡æ¬Šé‡èª¿æ•´ã€‘\")\n",
        "    results_a = run_all_tests(\n",
        "        test_cases=test_cases,\n",
        "        faiss_index=faiss_index,\n",
        "        chunks=chunks,\n",
        "        embedding_model=embedding_model,\n",
        "        bm25_index=bm25_index,\n",
        "        tokenized_corpus=tokenized_corpus,\n",
        "        llm_config=SELECTED_CONFIG,\n",
        "        use_priority_weighting=False,\n",
        "        enable_gate=False,\n",
        "    )\n",
        "\n",
        "    # æœ‰æ¬Šé‡ç‰ˆæœ¬\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"ã€ç‰ˆæœ¬ Bï¼šæœ‰æ¬Šé‡èª¿æ•´ã€‘\")\n",
        "    results_b = run_all_tests(\n",
        "        test_cases=test_cases,\n",
        "        faiss_index=faiss_index,\n",
        "        chunks=chunks,\n",
        "        embedding_model=embedding_model,\n",
        "        bm25_index=bm25_index,\n",
        "        tokenized_corpus=tokenized_corpus,\n",
        "        llm_config=SELECTED_CONFIG,\n",
        "        use_priority_weighting=True,\n",
        "        enable_gate=False,\n",
        "    )\n",
        "\n",
        "    # å°æ¯”åˆ†æ\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"ğŸ“Š å°æ¯”åˆ†æ\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    pass_a = sum(1 for r in results_a if r.get(\"passed\", False))\n",
        "    pass_b = sum(1 for r in results_b if r.get(\"passed\", False))\n",
        "    total = len(test_cases)\n",
        "\n",
        "    print(f\"\\næ•´é«”æº–ç¢ºç‡ï¼š\")\n",
        "    print(f\"  ç‰ˆæœ¬ Aï¼ˆç„¡æ¬Šé‡ï¼‰: {pass_a}/{total} ({pass_a/total*100:.1f}%)\")\n",
        "    print(f\"  ç‰ˆæœ¬ Bï¼ˆæœ‰æ¬Šé‡ï¼‰: {pass_b}/{total} ({pass_b/total*100:.1f}%)\")\n",
        "    print(f\"  æ”¹å–„: {pass_b - pass_a:+d} å€‹æ¡ˆä¾‹\")\n",
        "\n",
        "    # æ‰¾å‡ºå·®ç•°æ¡ˆä¾‹\n",
        "    print(f\"\\nå·®ç•°æ¡ˆä¾‹ï¼š\")\n",
        "    for test_id in test_cases.keys():\n",
        "        result_a = next((r for r in results_a if r[\"id\"] == test_id), None)\n",
        "        result_b = next((r for r in results_b if r[\"id\"] == test_id), None)\n",
        "\n",
        "        if result_a and result_b:\n",
        "            passed_a = result_a.get(\"passed\", False)\n",
        "            passed_b = result_b.get(\"passed\", False)\n",
        "\n",
        "            if passed_a != passed_b:\n",
        "                status = \"âœ… ä¿®æ­£\" if passed_b else \"âŒ é€€åŒ–\"\n",
        "                print(f\"  {test_id}: {status}\")\n",
        "                print(f\"    ç‰ˆæœ¬ A: {result_a.get('actual', 'N/A')}\")\n",
        "                print(f\"    ç‰ˆæœ¬ B: {result_b.get('actual', 'N/A')}\")\n",
        "                print(f\"    é æœŸ: {test_cases[test_id]['expected_assessment']}\")\n",
        "\n",
        "    return results_a, results_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpVG1pTZIibw"
      },
      "source": [
        "### 5.3 åŸ·è¡Œè¨ºæ–·ï¼ˆå–æ¶ˆè¨»è§£å³å¯åŸ·è¡Œï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTbEXWpEInZI"
      },
      "outputs": [],
      "source": [
        "# diagnose_single_case(\"8A\", show_chunks=False)\n",
        "# diagnose_single_case(\"8B\", show_chunks=False)\n",
        "# ab_test_comparison(END_TO_END_TEST_CASES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydigDzIVprYU"
      },
      "outputs": [],
      "source": [
        "# # åŸ·è¡Œ A/B æ¸¬è©¦ï¼ˆå¯é¸ï¼Œå› ç‚ºæœƒè·‘å…©æ¬¡æ‰€æœ‰æ¸¬è©¦ï¼‰\n",
        "# ab_test_comparison(END_TO_END_TEST_CASES)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4tYhbdzFeFCV",
        "fqELrbVzeNQ1",
        "fi__XUlneRLW",
        "YkxDeYhW5t7Y",
        "Ys_OH8AuQCHu",
        "3r5lj-wC6FT8",
        "9hsn9o1v6Wc8",
        "dGZx1JpjfWnU",
        "Jo22FdIOfeyd",
        "dEaMoM4GfkOi",
        "jBLJRgOzfs_y",
        "tOOfc1XdTyfQ",
        "VncezVFOgKkk",
        "cN3sifb3ZjUE",
        "abLfXt-xhCse",
        "38SVeGiChK3F"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPRtwAKjqfvTvRNNRy+dCmz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}