{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyh26x03/aml-redflags-rag/blob/main/build_data_v2_display.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTXsh23PzgEL"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "build_data.py â€” ç´¢å¼•å»ºç«‹ç®¡ç·š (Indexing Pipeline)\n",
        "\n",
        "è·è²¬ï¼šWrite / Create\n",
        "- è®€å– PDF æ–‡ä»¶\n",
        "- åˆ‡æˆå°æ®µè½ï¼ˆChunkingï¼‰\n",
        "- å»ºç«‹å‘é‡ç´¢å¼•ï¼ˆFAISSï¼‰\n",
        "- å»ºç«‹é—œéµå­—ç´¢å¼•ï¼ˆBM25ï¼‰\n",
        "- çµ±ä¸€å„²å­˜åˆ° Google Drive\n",
        "\n",
        "åŸ·è¡Œæ™‚æ©Ÿï¼š\n",
        "- é¦–æ¬¡è¨­ç½®\n",
        "- PDF æ–‡ä»¶æœ‰æ›´æ–°æ™‚\n",
        "- èª¿æ•´ chunking åƒæ•¸æˆ– embedding model æ™‚\n",
        "\n",
        "History:\n",
        "    - 2025-01-xx: v1 â€” åˆå§‹ç‰ˆæœ¬ï¼ˆchunk_size=300, ç„¡ metadata åˆ†å±¤ï¼‰\n",
        "    - 2025-02-09: v2 â€” åŠ å…¥ metadata åˆ†å±¤ã€retrieval_priorityã€doc_category\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWvKTUYHd1Om"
      },
      "source": [
        "# ğŸ”§ PART 0: SETUPï¼ˆç’°å¢ƒè¨­å®šï¼‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tYhbdzFeFCV"
      },
      "source": [
        "### 0.1 å®‰è£å¥—ä»¶\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgmniIRAdbSN"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf langchain-text-splitters sentence-transformers faiss-cpu rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqELrbVzeNQ1"
      },
      "source": [
        "### 0.2 Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVsDMr8YeO0e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi__XUlneRLW"
      },
      "source": [
        "### 0.3 è¨­å®šè·¯å¾‘èˆ‡åƒæ•¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPA4rd2hcIhY"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ardJiW530ZUN"
      },
      "outputs": [],
      "source": [
        "# --- è·¯å¾‘ ---\n",
        "SOURCE_DATA_DIR = \"/content/drive/MyDrive/AML/data\"\n",
        "INDEX_DIR = \"/content/drive/MyDrive/AML/index_v2\"\n",
        "\n",
        "CURRENT_VERSION = \"v2\" # å¯ä»¥ä¾æ“šéœ€æ±‚è¨­å®šç‰ˆæœ¬åç¨±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjs4EAP7eT6m"
      },
      "outputs": [],
      "source": [
        "# # è·¯å¾‘è¨­å®š\n",
        "# PDF_FOLDER = \"/content/drive/MyDrive/AML/data\"\n",
        "# BASE_INDEX_ROOT_DIR = \"/content/drive/MyDrive/AML\" # ç´¢å¼•å„²å­˜çš„æ ¹ç›®éŒ„\n",
        "# CURRENT_INDEX_VERSION = \"index_v2\" # å¯ä»¥ä¾æ“šéœ€æ±‚è¨­å®šç‰ˆæœ¬åç¨±ï¼Œä¾‹å¦‚ \"fatf_va_v1\"\n",
        "# INDEX_PATH = f\"{BASE_INDEX_ROOT_DIR}/{CURRENT_INDEX_VERSION}\" # è¨ˆç®—å‡ºå®Œæ•´çš„ç´¢å¼•è·¯å¾‘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VSVJ2sK0Znt"
      },
      "outputs": [],
      "source": [
        "# --- Embedding æ¨¡å‹ ---\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mICdoD7WcL2W"
      },
      "outputs": [],
      "source": [
        "# --- Chunking åƒæ•¸ ---\n",
        "CHUNK_SIZE = 400\n",
        "CHUNK_OVERLAP = 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkjBVvdqcTGe"
      },
      "outputs": [],
      "source": [
        "# --- é©—è­‰ ---\n",
        "assert os.path.exists(SOURCE_DATA_DIR), f\"âŒ è³‡æ–™è·¯å¾‘ä¸å­˜åœ¨: {SOURCE_DATA_DIR}\"\n",
        "pdf_files = sorted([f for f in os.listdir(SOURCE_DATA_DIR) if f.endswith(\".pdf\")])\n",
        "assert len(pdf_files) > 0, f\"âŒ {SOURCE_DATA_DIR} ä¸­æ‰¾ä¸åˆ°ä»»ä½• PDF\"\n",
        "print(f\"âœ… Config è¼‰å…¥å®Œæˆ\")\n",
        "print(f\"   è³‡æ–™è·¯å¾‘: {SOURCE_DATA_DIR} ({len(pdf_files)} å€‹ PDF)\")\n",
        "for f in pdf_files:\n",
        "    print(f\"      ğŸ“„ {f}\")\n",
        "print(f\"   ç´¢å¼•è¼¸å‡º: {INDEX_DIR}\")\n",
        "print(f\"   Embedding: {EMBEDDING_MODEL_NAME}\")\n",
        "print(f\"   Chunk: size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK5jiGzjeTE2"
      },
      "source": [
        "# ğŸ“š PART 1: è³‡æ–™æº–å‚™èˆ‡ç´¢å¼•å»ºç«‹ (Indexing Pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_3pMrM2ev73"
      },
      "source": [
        "è·è²¬ï¼šWrite/Create\n",
        "- è®€å– PDF æ–‡ä»¶\n",
        "- åˆ‡æˆå°æ®µè½ï¼ˆChunkingï¼‰\n",
        "- å»ºç«‹å‘é‡ç´¢å¼•ï¼ˆFAISSï¼‰\n",
        "- å»ºç«‹é—œéµå­—ç´¢å¼•ï¼ˆBM25ï¼‰\n",
        "- çµ±ä¸€å„²å­˜åˆ° Google Drive\n",
        "\n",
        "\n",
        "åŸ·è¡Œæ™‚æ©Ÿï¼š\n",
        "- é¦–æ¬¡è¨­ç½®\n",
        "- PDF æ–‡ä»¶æœ‰æ›´æ–°æ™‚\n",
        "- èª¿æ•´ chunking åƒæ•¸æ™‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcCxQ1bfe2ug"
      },
      "source": [
        "### 1.1 è®€å– PDF + Metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYAEvmmF2n_Q"
      },
      "source": [
        "```\n",
        "æ ¸å¿ƒå±¤ï¼ˆcoreï¼‰\n",
        "â”œâ”€ FATF ç´…æ——æ¨™æº–           priority: 1.0\n",
        "â””â”€ å°ç£ AML æ³•è¦ç¸½è¦½       priority: 1.0\n",
        "\n",
        "èªæ„æ©‹æ¨‘å±¤ï¼ˆknowledge_bridgeï¼‰\n",
        "â”œâ”€ è¨“ç·´æŠ•å½±ç‰‡              priority: 0.8  â† ä½ é—œæ³¨çš„é€™å€‹\n",
        "â”œâ”€ æ•™å¸«æ‰‹å†Š                priority: 0.8\n",
        "â””â”€ é˜²åˆ¶æŒ‡å—                priority: 0.8\n",
        "\n",
        "é ˜åŸŸç´°å‰‡å±¤ï¼ˆsector_specificï¼‰\n",
        "â”œâ”€ è™›æ“¬è³‡ç”¢ç´…æ——            priority: 0.9\n",
        "â”œâ”€ éŠ€è¡Œæ¥­ç´°å‰‡              priority: 0.9\n",
        "â””â”€ è­‰åˆ¸æ¥­ Q&A              priority: 0.9\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆï¼Ÿ\n",
        "\n",
        "å•é¡Œï¼šè¨“ç·´æŠ•å½±ç‰‡çš„ã€Œé›™é‡è§’è‰²ã€\n",
        "```\n",
        "tw_aml_training_slides.pdf çš„å…§å®¹ï¼š\n",
        "â”œâ”€ åŸºæœ¬æ¦‚å¿µï¼ˆä»€éº¼æ˜¯æ´—éŒ¢ï¼Ÿï¼‰        â† é€™æ˜¯æ©‹æ¨‘\n",
        "â”œâ”€ é‡‘èæ©Ÿæ§‹å¯¦å‹™ï¼ˆé–‹æˆ¶å¯©æŸ¥ï¼‰        â† é€™ä¹Ÿæ˜¯æ©‹æ¨‘\n",
        "â””â”€ æ¡ˆä¾‹èªªæ˜ï¼ˆ400è¬åˆ†æ‰¹å­˜æ¬¾ï¼‰       â† é€™é‚„æ˜¯æ©‹æ¨‘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GpKSqKb3yc6"
      },
      "outputs": [],
      "source": [
        "from pypdf import PdfReader\n",
        "import os\n",
        "\n",
        "def get_pdf_metadata(pdf_name):\n",
        "    \"\"\"\n",
        "    æ ¹æ“šæª”åå›å‚³å°æ‡‰çš„ metadata\n",
        "\n",
        "    åˆ†å±¤é‚è¼¯ï¼š\n",
        "    - core: æ¬Šå¨æ³•è¦/åœ‹éš›æ¨™æº–ï¼ˆFATFã€å°ç£æ³•è¦ï¼‰\n",
        "    - knowledge_bridge: ç°¡åŒ–æ•™å­¸å…§å®¹ï¼ˆè¨“ç·´æ•™æã€æŒ‡å—ï¼‰\n",
        "    - sector_specific: é ˜åŸŸç´°å‰‡ï¼ˆè™›æ“¬è³‡ç”¢ã€éŠ€è¡Œã€è­‰åˆ¸ï¼‰\n",
        "    \"\"\"\n",
        "    metadata = {\n",
        "        \"source\": \"Unknown\",\n",
        "        \"jurisdiction\": \"Unknown\",\n",
        "        \"doc_type\": \"Unknown\",\n",
        "        \"language\": \"en\",\n",
        "        \"doc_category\": \"unknown\",\n",
        "        \"retrieval_priority\": 1.0,\n",
        "        \"explanation_style\": \"neutral\"\n",
        "    }\n",
        "\n",
        "    # === CORE LAYER ===\n",
        "    if \"fatf_tbm_laundering_red_flags\" in pdf_name:\n",
        "        metadata.update({\n",
        "            \"source\": \"FATF\",\n",
        "            \"jurisdiction\": \"International\",\n",
        "            \"doc_type\": \"red_flag\",\n",
        "            \"language\": \"en\",\n",
        "            \"doc_category\": \"core\",\n",
        "            \"retrieval_priority\": 1.0,\n",
        "            \"explanation_style\": \"authoritative\"\n",
        "        })\n",
        "\n",
        "    # === SECTOR LAYER ===\n",
        "    elif \"fatf_virtual_assets_red_flags\" in pdf_name:\n",
        "        metadata.update({\n",
        "            \"source\": \"FATF\",\n",
        "            \"jurisdiction\": \"International\",\n",
        "            \"doc_type\": \"red_flag\",\n",
        "            \"language\": \"en\",\n",
        "            \"doc_category\": \"sector_specific\",\n",
        "            \"retrieval_priority\": 0.9,\n",
        "            \"explanation_style\": \"authoritative\"\n",
        "        })\n",
        "\n",
        "    # === KNOWLEDGE BRIDGE LAYER ===\n",
        "    elif \"tw_aml_training_slides\" in pdf_name:\n",
        "        metadata.update({\n",
        "            \"source\": \"TW_Gov\",\n",
        "            \"jurisdiction\": \"Taiwan\",\n",
        "            \"doc_type\": \"training\",\n",
        "            \"language\": \"zh\",\n",
        "            \"doc_category\": \"knowledge_bridge\",\n",
        "            \"retrieval_priority\": 0.8,\n",
        "            \"explanation_style\": \"simplified\"\n",
        "        })\n",
        "\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAxMRnaHesE6"
      },
      "outputs": [],
      "source": [
        "def load_pdfs(folder_path):\n",
        "    \"\"\"\n",
        "    è®€å–è³‡æ–™å¤¾ä¸­æ‰€æœ‰ PDFï¼Œä¸¦é™„ä¸Š metadata\n",
        "\n",
        "    Args:\n",
        "        folder_path: PDF è³‡æ–™å¤¾è·¯å¾‘\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: æ¯å€‹ dict ä»£è¡¨ä¸€é ï¼ŒåŒ…å« text + metadata\n",
        "    \"\"\"\n",
        "\n",
        "    pdf_paths = [\n",
        "        os.path.join(folder_path, f)\n",
        "        for f in sorted(os.listdir(folder_path))\n",
        "        if f.endswith(\".pdf\")\n",
        "    ]\n",
        "\n",
        "    if not pdf_paths:\n",
        "        raise FileNotFoundError(f\"åœ¨ {folder_path} ä¸­æ‰¾ä¸åˆ°ä»»ä½• PDF æª”æ¡ˆ\")\n",
        "\n",
        "    parsed_pdf_pages = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        pdf_name = os.path.basename(pdf_path)\n",
        "        metadata = get_pdf_metadata(pdf_name)\n",
        "\n",
        "        for i, page in enumerate(reader.pages, start=1):\n",
        "            text = page.extract_text()\n",
        "            if text and text.strip():  # è·³éç©ºç™½é \n",
        "                parsed_pdf_pages.append({\n",
        "                    \"pdf_name\": pdf_name,\n",
        "                    \"page\": i,\n",
        "                    \"text\": text,\n",
        "                    **metadata,\n",
        "                })\n",
        "\n",
        "    return parsed_pdf_pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwAM1MB4fK-A"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# === åŸ·è¡Œ ===\n",
        "print(\"\\nğŸ“š 1.1 è®€å– PDF...\")\n",
        "parsed_pdf_pages = load_pdfs(SOURCE_DATA_DIR)\n",
        "print(f\"   âœ… è®€å–äº† {len(parsed_pdf_pages)} é \")\n",
        "\n",
        "page_counts = Counter(p[\"pdf_name\"] for p in parsed_pdf_pages)\n",
        "for name, count in page_counts.items():\n",
        "    meta = get_pdf_metadata(name)\n",
        "    print(f\"   ğŸ“„ {name}: {count} é  | category={meta['doc_category']} | priority={meta['retrieval_priority']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6v2s4hSfAGw"
      },
      "source": [
        "### 1.2 Chunkingï¼ˆåˆ‡æ®µè½ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bic3RPBefHSX"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def create_chunks(\n",
        "    pages: list,\n",
        "    chunk_size: int = CHUNK_SIZE,\n",
        "    chunk_overlap: int = CHUNK_OVERLAP,\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    æŠŠé é¢åˆ‡æˆå°æ®µè½\n",
        "\n",
        "    Args:\n",
        "        pages: load_pdfs() çš„è¼¸å‡º\n",
        "        chunk_size: æ¯å€‹ chunk çš„æœ€å¤§å­—å…ƒæ•¸\n",
        "        chunk_overlap: chunk ä¹‹é–“çš„é‡ç–Šå­—å…ƒæ•¸\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: æ¨™æº–åŒ–çš„ chunk åˆ—è¡¨ï¼Œæ¯å€‹ dict åŒ…å« text + metadata\n",
        "    \"\"\"\n",
        "    # ä¸­è‹±æ–‡éƒ½é©ç”¨çš„åˆ†éš”ç¬¦\n",
        "    separators = [\"\\n\\n\", \"\\n\", \"ã€‚\", \".\", \"ï¼\", \"!\", \"ï¼Ÿ\", \"?\", \"ï¼›\", \";\", \" \"]\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=separators,\n",
        "    )\n",
        "\n",
        "    chunks = []\n",
        "    for page in pages:\n",
        "        splits = splitter.split_text(page[\"text\"])\n",
        "        for j, split_text in enumerate(splits):\n",
        "            chunks.append({\n",
        "                \"text\": split_text,\n",
        "                \"page\": page[\"page\"],\n",
        "                \"chunk_id\": f\"{page['pdf_name']}_p{page['page']}_c{j}\",\n",
        "                \"source\": page[\"source\"],\n",
        "                \"language\": page[\"language\"],\n",
        "                \"doc_type\": page[\"doc_type\"],\n",
        "                \"retrieval_priority\": page.get(\"retrieval_priority\", 1.0),\n",
        "                \"doc_category\": page.get(\"doc_category\", \"unknown\"),\n",
        "                \"explanation_style\": page.get(\"explanation_style\", \"neutral\"),\n",
        "            })\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyf-PQIffM74"
      },
      "outputs": [],
      "source": [
        "# === åŸ·è¡Œ ===\n",
        "print(\"\\nâœ‚ï¸ 1.2 Chunking...\")\n",
        "chunks = create_chunks(parsed_pdf_pages)\n",
        "print(f\"   âœ… ç”¢ç”Ÿäº† {len(chunks)} å€‹ chunks\")\n",
        "\n",
        "chunk_cats = Counter(c[\"doc_category\"] for c in chunks)\n",
        "for cat, count in chunk_cats.items():\n",
        "    print(f\"   ğŸ“¦ {cat}: {count} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-45lyiTvfP5H"
      },
      "source": [
        "### 1.3 Embedding + FAISS Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtfQPSlim9io"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_faiss_index(chunks: list, embedding_model_name: str):\n",
        "    \"\"\"\n",
        "    å»ºç«‹å‘é‡ç´¢å¼•\n",
        "\n",
        "    Args:\n",
        "        chunks: chunk åˆ—è¡¨\n",
        "        embedding_model_name: sentence-transformers æ¨¡å‹åç¨±\n",
        "\n",
        "    Returns:\n",
        "        tuple: (embedding_model, faiss_index)\n",
        "    \"\"\"\n",
        "    print(f\"   è¼‰å…¥æ¨¡å‹: {embedding_model_name}\")\n",
        "    embedding_model = SentenceTransformer(embedding_model_name)\n",
        "\n",
        "    # ç”¢ç”Ÿ embeddings\n",
        "    texts = [c[\"text\"] for c in chunks]\n",
        "    embeddings = embedding_model.encode(\n",
        "        texts,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True,\n",
        "    )\n",
        "\n",
        "    # å»ºç«‹ FAISS indexï¼ˆä½¿ç”¨ Inner Productï¼Œå› ç‚ºå·²ç¶“ normalizeï¼‰\n",
        "    dim = embeddings.shape[1]\n",
        "    faiss_index = faiss.IndexFlatIP(dim)\n",
        "    faiss_index.add(np.array(embeddings, dtype=\"float32\"))\n",
        "\n",
        "    return embedding_model, faiss_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emGjKVRbfTaG"
      },
      "outputs": [],
      "source": [
        "# åŸ·è¡Œ\n",
        "print(\"\\nğŸ§  1.3 å»ºç«‹ FAISS Indexï¼ˆå‘é‡ç´¢å¼•ï¼‰...\")\n",
        "embedding_model, faiss_index = create_faiss_index(chunks, EMBEDDING_MODEL_NAME)\n",
        "print(f\"   âœ… å»ºç«‹å®Œæˆï¼Œå…± {faiss_index.ntotal} å€‹å‘é‡ï¼Œç¶­åº¦ {faiss_index.d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxJsQDVLndsY"
      },
      "source": [
        "###1.4 BM25 Indexï¼ˆé—œéµå­—ç´¢å¼•ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0_NNf9cniGQ"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import jieba\n",
        "\n",
        "\n",
        "def create_bm25_index(chunks: list):\n",
        "    \"\"\"\n",
        "    å»ºç«‹ BM25 é—œéµå­—ç´¢å¼•\n",
        "\n",
        "    Args:\n",
        "        chunks: chunk åˆ—è¡¨ï¼ˆéœ€è¦æœ‰ text å’Œ language æ¬„ä½ï¼‰\n",
        "\n",
        "    Returns:\n",
        "        tuple: (bm25_index, tokenized_corpus)\n",
        "    \"\"\"\n",
        "    tokenized_corpus = []\n",
        "\n",
        "    for c in chunks:\n",
        "        if c.get(\"language\") == \"zh\":\n",
        "            tokens = list(jieba.cut(c[\"text\"]))\n",
        "        else:\n",
        "            tokens = c[\"text\"].lower().split()\n",
        "        tokenized_corpus.append(tokens)\n",
        "\n",
        "    bm25_index = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "    return bm25_index, tokenized_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJz4X0sUnkxY"
      },
      "outputs": [],
      "source": [
        "# åŸ·è¡Œ\n",
        "print(\"\\nğŸ“ 1.4 å»ºç«‹ BM25 Indexï¼ˆé—œéµå­—ç´¢å¼•ï¼‰...\")\n",
        "bm25_index, tokenized_corpus = create_bm25_index(chunks)\n",
        "print(f\"   âœ… å»ºç«‹å®Œæˆï¼Œå…± {len(tokenized_corpus)} å€‹æ–‡ä»¶\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEdW61M3xMuu"
      },
      "source": [
        "###1.5 å„²å­˜åˆ° Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5zubSwpxTZH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def save_all_indexes(\n",
        "    base_dir: str,\n",
        "    faiss_index,\n",
        "    chunks: list,\n",
        "    bm25_index,\n",
        "    tokenized_corpus: list,\n",
        "    embedding_model_name: str,\n",
        "    chunk_size: int,\n",
        "    chunk_overlap: int,\n",
        "    version_name: str = CURRENT_VERSION\n",
        "):\n",
        "    \"\"\"\n",
        "    çµ±ä¸€å„²å­˜æ‰€æœ‰ç´¢å¼•å’Œè³‡æ–™ï¼Œæ”¯æ´ç‰ˆæœ¬ç®¡ç†ã€‚\n",
        "\n",
        "    å„²å­˜çš„æª”æ¡ˆï¼š\n",
        "    - faiss_index.bin: FAISS å‘é‡ç´¢å¼•\n",
        "    - chunks.json: åŸå§‹ chunksï¼ˆJSON æ ¼å¼ï¼Œæ–¹ä¾¿æª¢è¦–ï¼‰\n",
        "    - bm25_index.pkl: BM25 ç´¢å¼•\n",
        "    - tokenized_corpus.pkl: åˆ†è©å¾Œçš„èªæ–™\n",
        "    - metadata.json: ç´¢å¼•çš„ metadataï¼ˆç‰ˆæœ¬ã€å»ºç«‹æ™‚é–“ç­‰ï¼‰\n",
        "\n",
        "    Args:\n",
        "        base_dir: ç´¢å¼•æ ¹ç›®éŒ„ï¼ˆå¦‚ /content/drive/MyDrive/AML/indicesï¼‰\n",
        "        version_name: ç‰ˆæœ¬åç¨±ï¼ˆå¦‚ \"v2\"ï¼‰\n",
        "        faiss_index: FAISS ç´¢å¼•ç‰©ä»¶\n",
        "        chunks: chunk åˆ—è¡¨\n",
        "        bm25_index: BM25 ç´¢å¼•ç‰©ä»¶\n",
        "        tokenized_corpus: åˆ†è©å¾Œçš„èªæ–™\n",
        "        embedding_model_name: ä½¿ç”¨çš„ embedding æ¨¡å‹åç¨±\n",
        "        chunk_size: chunk å¤§å°\n",
        "        chunk_overlap: chunk é‡ç–Š\n",
        "    \"\"\"\n",
        "    full_path = Path(base_dir) / version_name\n",
        "    full_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"   å„²å­˜è·¯å¾‘: {full_path}\")\n",
        "\n",
        "    # 1. FAISS Index\n",
        "    faiss.write_index(faiss_index, str(full_path / \"faiss_index.bin\"))\n",
        "    print(f\"   âœ… faiss_index.bin\")\n",
        "\n",
        "    # 2. Chunks\n",
        "    with open(full_path / \"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"   âœ… chunks.json ({len(chunks)} chunks)\")\n",
        "\n",
        "    # 3. BM25 Index\n",
        "    with open(full_path / \"bm25_index.pkl\", \"wb\") as f:\n",
        "        pickle.dump(bm25_index, f)\n",
        "    print(f\"   âœ… bm25_index.pkl\")\n",
        "\n",
        "    # 4. Tokenized Corpus\n",
        "    with open(full_path / \"tokenized_corpus.pkl\", \"wb\") as f:\n",
        "        pickle.dump(tokenized_corpus, f)\n",
        "    print(f\"   âœ… tokenized_corpus.pkl\")\n",
        "\n",
        "    # 5. Metadata\n",
        "    metadata = {\n",
        "        \"version\": version_name,\n",
        "        \"created_at\": datetime.now().isoformat(),\n",
        "        \"config\": {\n",
        "            \"embedding_model\": embedding_model_name,\n",
        "            \"chunk_size\": chunk_size,\n",
        "            \"chunk_overlap\": chunk_overlap,\n",
        "        },\n",
        "        \"stats\": {\n",
        "            \"total_chunks\": len(chunks),\n",
        "            \"total_vectors\": faiss_index.ntotal,\n",
        "            \"vector_dimension\": faiss_index.d,\n",
        "        },\n",
        "    }\n",
        "    with open(full_path / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"   âœ… metadata.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82Na-6r4YWCp"
      },
      "outputs": [],
      "source": [
        "# === åŸ·è¡Œ ===\n",
        "print(\"\\nğŸ’¾ 1.5 å„²å­˜åˆ° Google Drive...\")\n",
        "save_all_indexes(\n",
        "    base_dir=INDEX_DIR,\n",
        "    faiss_index=faiss_index,\n",
        "    chunks=chunks,\n",
        "    bm25_index=bm25_index,\n",
        "    tokenized_corpus=tokenized_corpus,\n",
        "    embedding_model_name=EMBEDDING_MODEL_NAME,\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IW4uTvTxlaJ"
      },
      "outputs": [],
      "source": [
        "# é©—è­‰\n",
        "print(\"\\nğŸ“‹ å„²å­˜çµæœé©—è­‰ï¼š\")\n",
        "print(\"-\" * 40)\n",
        "for f in sorted(os.listdir(INDEX_DIR)):\n",
        "    size = os.path.getsize(os.path.join(INDEX_DIR, f))\n",
        "    print(f\"   {f}: {size:,} bytes\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… PART 1 å®Œæˆï¼æ‰€æœ‰ç´¢å¼•å·²å„²å­˜ã€‚\")\n",
        "print(\"   ä¸‹æ¬¡å¯¦é©—åªéœ€è¦åœ¨ experiment_rag_v2 ä¸­è¼‰å…¥å³å¯ã€‚\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFTJ-OxuJ4m8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ARqj3U3J5Tp"
      },
      "outputs": [],
      "source": [
        "# è¤‡è£½åŸå§‹ notebook æˆå±•ç¤ºç‰ˆæœ¬\n",
        "!cp rag_project.ipynb rag_project_display.ipynb\n",
        "\n",
        "# æ¸…æ‰å±•ç¤ºç‰ˆæœ¬çš„è¼¸å‡º\n",
        "!jupyter nbconvert --clear-output --inplace rag_project_display.ipynb\n",
        "\n",
        "# åª commit å±•ç¤ºç‰ˆæœ¬\n",
        "!git add rag_project_display.ipynb\n",
        "!git commit -m \"update: clean display version\"\n",
        "!git push"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNr1ZaLhoPQ5VdnC/y2+fJ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}